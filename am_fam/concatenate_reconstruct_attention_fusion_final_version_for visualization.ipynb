{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unable-syntax",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.style.use('ggplot')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import sys, os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch, torchvision\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, roc_curve, auc\n",
    "from itertools import cycle\n",
    "from itertools import chain\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.metrics import specificity_score\n",
    "from dalib.modules import WarmStartGradientReverseLayer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "tamil-swiss",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-30 11:10:07.369105: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-30 11:10:07.965191: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from derm7pt.dataset import Derm7PtDataset, Derm7PtDatasetGroupInfrequent\n",
    "from derm7pt.vis import plot_confusion\n",
    "from derm7pt.kerasutils import deep_features\n",
    "dir_release = '../data/release_v0'\n",
    "dir_meta = os.path.join(dir_release, 'meta')\n",
    "dir_images = os.path.join(dir_release, 'images')\n",
    "meta_df = pd.read_csv(os.path.join(dir_meta, 'meta.csv'))\n",
    "train_indexes = list(pd.read_csv(os.path.join(dir_meta, 'train_indexes.csv'))['indexes'])\n",
    "valid_indexes = list(pd.read_csv(os.path.join(dir_meta, 'valid_indexes.csv'))['indexes'])\n",
    "test_indexes = list(pd.read_csv(os.path.join(dir_meta, 'test_indexes.csv'))['indexes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "saved-trading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The full dataset before any grouping of the labels.\n",
    "derm_data = Derm7PtDataset(dir_images=dir_images, \n",
    "                        metadata_df=meta_df.copy(), # Copy as is modified.\n",
    "                        train_indexes=train_indexes, valid_indexes=valid_indexes, \n",
    "                        test_indexes=test_indexes)\n",
    "\n",
    "# The dataset after grouping infrequent labels.\n",
    "derm_data_group = Derm7PtDatasetGroupInfrequent(dir_images=dir_images, \n",
    "                                             metadata_df=meta_df.copy(), # Copy as is modified.\n",
    "                                             train_indexes=train_indexes, \n",
    "                                             valid_indexes=valid_indexes, \n",
    "                                             test_indexes=test_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "published-fellow",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cases: 1011\n",
      "Number of cases to train: 413\n",
      "Number of cases to validate: 203\n",
      "Number of cases to test: 395\n"
     ]
    }
   ],
   "source": [
    "derm_data.dataset_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "regular-sociology",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '../data/release_v0/images/'\n",
    "BCC = ['basal cell carcinoma'] \n",
    "NEV = ['blue nevus', 'clark nevus', 'combined nevus', 'congenital nevus', 'dermal nevus', 'recurrent nevus', 'reed or spitz nevus']\n",
    "MEL = ['melanoma', 'melanoma (in situ)', 'melanoma (less than 0.76 mm)', 'melanoma (0.76 to 1.5 mm)', 'melanoma (more than 1.5 mm)', 'melanoma metastasis' ]\n",
    "MISC = ['dermatofibroma', 'lentigo', 'melanosis', 'miscellaneous', 'vascular lesion']\n",
    "SK = ['seborrheic keratosis']\n",
    "PN = {'absent':0, 'typical':1, 'atypical':2}\n",
    "STR = {'absent':0, 'regular':1, 'irregular':2}\n",
    "PIG = {'absent':0, 'diffuse regular':1, 'localized regular':1, 'diffuse irregular':2, 'localized irregular':2}\n",
    "RS = {'absent':0, 'blue areas':1, 'white areas':1, 'combinations':1}\n",
    "DaG = {'absent':0, 'regular':1, 'irregular':2}\n",
    "BWV = {'absent':0, 'present':1}\n",
    "VS = {'absent':0, 'arborizing':1, 'comma':1, 'hairpin':1, 'within regression':1, 'wreath':1, 'dotted':2, 'linear irregular':2}\n",
    "def get_diag_label(diag):\n",
    "    if diag in BCC:\n",
    "        label = 0\n",
    "    elif diag in NEV:\n",
    "        label = 1\n",
    "    elif diag in MEL:\n",
    "        label = 2\n",
    "    elif diag in MISC:\n",
    "        label = 3\n",
    "    elif diag in SK:\n",
    "        label = 4\n",
    "    if label == None:\n",
    "        print(\"Error!\")\n",
    "    else:\n",
    "        return label\n",
    "def get_7point_label(point_criteria):\n",
    "    label0 = PN[point_criteria[0]]\n",
    "    label1 = STR[point_criteria[1]]\n",
    "    label2 = PIG[point_criteria[2]]\n",
    "    label3 = RS[point_criteria[3]]\n",
    "    label4 = DaG[point_criteria[4]]\n",
    "    label5 = BWV[point_criteria[5]]\n",
    "    label6 = VS[point_criteria[6]]\n",
    "    return [label0, label1, label2, label3, label4, label5, label6]\n",
    "clinic_train = []\n",
    "clinic_validate = []\n",
    "clinic_test = []\n",
    "dermoscopic_train = []\n",
    "dermoscopic_validate = []\n",
    "dermoscopic_test = []\n",
    "label_train_diag = []\n",
    "label_validate_diag = []\n",
    "label_test_diag = []\n",
    "label_train_crit = []\n",
    "label_validate_crit = []\n",
    "label_test_crit = []\n",
    "for index, row in meta_df.iterrows():\n",
    "    c_img = row[15]\n",
    "    d_img = row[16]\n",
    "    diag = row[1]\n",
    "    p_n = row[3]\n",
    "    s_t_r = row[4]\n",
    "    p_i_g = row[5]\n",
    "    r_s = row[6]\n",
    "    d_a_g = row[7]\n",
    "    b_w_v = row[8]\n",
    "    v_s = row[9]\n",
    "    point_criteria = [p_n, s_t_r, p_i_g, r_s, d_a_g, b_w_v, v_s]\n",
    "    if d_img == 'FCl/Fcl068.jpg':\n",
    "        d_img = 'FCL/Fcl068.jpg'\n",
    "\n",
    "    if index in train_indexes:        \n",
    "        clinic_train.append(img_path + c_img)\n",
    "        dermoscopic_train.append(img_path + d_img)\n",
    "        label_train_diag.append(get_diag_label(diag))\n",
    "        label_train_crit.append(get_7point_label(point_criteria))\n",
    "    elif index in valid_indexes:\n",
    "        clinic_validate.append(img_path + c_img)\n",
    "        dermoscopic_validate.append(img_path + d_img)\n",
    "        label_validate_diag.append(get_diag_label(diag))\n",
    "        label_validate_crit.append(get_7point_label(point_criteria))\n",
    "    elif index in test_indexes:\n",
    "        clinic_test.append(img_path + c_img)\n",
    "        dermoscopic_test.append(img_path + d_img)\n",
    "        label_test_diag.append(get_diag_label(diag))\n",
    "        label_test_crit.append(get_7point_label(point_criteria))\n",
    "    else:\n",
    "        print(\"There is an error need to be fixed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "arranged-background",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413 203 395\n"
     ]
    }
   ],
   "source": [
    "print(len(clinic_train), len(clinic_validate), len(clinic_test))\n",
    "# print(clinic_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "optical-reduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_c = Image.open(self.data.c_path[idx])\n",
    "        img_c = img_c.convert(\"RGB\")\n",
    "        img_c = self.transform(img_c)\n",
    "        img_d = Image.open(self.data.d_path[idx])\n",
    "        img_d = img_d.convert(\"RGB\")\n",
    "        img_d = self.transform(img_d)\n",
    "        \n",
    "        label_diag_i = np.array(self.data.lab_diag[idx])\n",
    "        \n",
    "        label_crit_i = np.array(self.data.lab_crit[idx])\n",
    "        \n",
    "        return img_c, img_d, label_diag_i, label_crit_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "rural-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.Resize([299, 299]),\n",
    "                                       # transforms.Pad(padding=10, fill=(255, 176, 145)),\n",
    "                                       transforms.RandomCrop([299, 299], padding=20, padding_mode='edge'),\n",
    "                                       transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                       transforms.RandomRotation([-45, 45]),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize((0.5 , 0.5 , 0.5), (0.5 , 0.5 , 0.5))])\n",
    "test_transforms = transforms.Compose([transforms.Resize([299, 299]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5 , 0.5 , 0.5), (0.5 , 0.5 , 0.5))\n",
    "])\n",
    "image_transforms = {'train':train_transforms, 'test':test_transforms}\n",
    "\n",
    "train = list(zip(clinic_train, dermoscopic_train, label_train_diag, label_train_crit))\n",
    "train_df = pd.DataFrame(train, columns=['c_path','d_path','lab_diag', 'lab_crit'])\n",
    "train_dataset = MyDataset(train_df, transform=image_transforms['train'])\n",
    "\n",
    "validate = list(zip(clinic_validate, dermoscopic_validate, label_validate_diag, label_validate_crit))\n",
    "validate_df = pd.DataFrame(validate, columns=['c_path','d_path','lab_diag', 'lab_crit'])\n",
    "validate_dataset = MyDataset(validate_df, transform=image_transforms['test'])\n",
    "\n",
    "test = list(zip(clinic_test, dermoscopic_test, label_test_diag, label_test_crit))\n",
    "test_df = pd.DataFrame(test, columns=['c_path','d_path','lab_diag', 'lab_crit'])\n",
    "test_dataset = MyDataset(test_df, transform=image_transforms['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea8c948d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_path</th>\n",
       "      <th>d_path</th>\n",
       "      <th>lab_diag</th>\n",
       "      <th>lab_crit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/release_v0/images/NEL/NEL025.JPG</td>\n",
       "      <td>../data/release_v0/images/NEL/Nel026.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/release_v0/images/NEL/Nel032.jpg</td>\n",
       "      <td>../data/release_v0/images/NEL/Nel033.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 2, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/release_v0/images/NEL/NEL036.JPG</td>\n",
       "      <td>../data/release_v0/images/NEL/Nel037.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 2, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/release_v0/images/NEL/Nel084.jpg</td>\n",
       "      <td>../data/release_v0/images/NEL/Nel085.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 2, 0, 2, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/release_v0/images/NEL/NEL088.JPG</td>\n",
       "      <td>../data/release_v0/images/NEL/Nel089.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 2, 0, 2, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>../data/release_v0/images/NEL/Nel067.jpg</td>\n",
       "      <td>../data/release_v0/images/NEL/Nel066.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>../data/release_v0/images/NEL/Nel069.jpg</td>\n",
       "      <td>../data/release_v0/images/NEL/Nel068.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>../data/release_v0/images/NEL/Nel070.jpg</td>\n",
       "      <td>../data/release_v0/images/NEL/Nel071.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>../data/release_v0/images/Ggl/Ggl011.jpg</td>\n",
       "      <td>../data/release_v0/images/Ggl/Ggl012.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>../data/release_v0/images/Fhl/Fhl059.jpg</td>\n",
       "      <td>../data/release_v0/images/Fhl/Fhl060.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>413 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       c_path  \\\n",
       "0    ../data/release_v0/images/NEL/NEL025.JPG   \n",
       "1    ../data/release_v0/images/NEL/Nel032.jpg   \n",
       "2    ../data/release_v0/images/NEL/NEL036.JPG   \n",
       "3    ../data/release_v0/images/NEL/Nel084.jpg   \n",
       "4    ../data/release_v0/images/NEL/NEL088.JPG   \n",
       "..                                        ...   \n",
       "408  ../data/release_v0/images/NEL/Nel067.jpg   \n",
       "409  ../data/release_v0/images/NEL/Nel069.jpg   \n",
       "410  ../data/release_v0/images/NEL/Nel070.jpg   \n",
       "411  ../data/release_v0/images/Ggl/Ggl011.jpg   \n",
       "412  ../data/release_v0/images/Fhl/Fhl059.jpg   \n",
       "\n",
       "                                       d_path  lab_diag               lab_crit  \n",
       "0    ../data/release_v0/images/NEL/Nel026.jpg         0  [0, 0, 0, 0, 0, 0, 1]  \n",
       "1    ../data/release_v0/images/NEL/Nel033.jpg         0  [0, 0, 0, 0, 2, 0, 1]  \n",
       "2    ../data/release_v0/images/NEL/Nel037.jpg         0  [0, 0, 2, 0, 0, 0, 0]  \n",
       "3    ../data/release_v0/images/NEL/Nel085.jpg         0  [0, 0, 2, 0, 2, 1, 0]  \n",
       "4    ../data/release_v0/images/NEL/Nel089.jpg         0  [0, 0, 2, 0, 2, 0, 1]  \n",
       "..                                        ...       ...                    ...  \n",
       "408  ../data/release_v0/images/NEL/Nel066.jpg         3  [0, 0, 0, 0, 0, 0, 0]  \n",
       "409  ../data/release_v0/images/NEL/Nel068.jpg         3  [0, 0, 0, 0, 0, 0, 0]  \n",
       "410  ../data/release_v0/images/NEL/Nel071.jpg         3  [0, 0, 0, 0, 0, 0, 0]  \n",
       "411  ../data/release_v0/images/Ggl/Ggl012.jpg         3  [0, 0, 0, 0, 0, 0, 0]  \n",
       "412  ../data/release_v0/images/Fhl/Fhl060.jpg         3  [0, 0, 0, 0, 0, 0, 0]  \n",
       "\n",
       "[413 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "false-division",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_auc(pre, true, show = False):\n",
    "    auc_all = {}\n",
    "    \n",
    "    plt.rc('font',family='Times New Roman')\n",
    "    font1 = {'family': 'Times New Roman', 'weight': 'normal', 'size': 10,}\n",
    "    for key in pre.keys():\n",
    "        n_classes = np.array(pre[key]).shape[-1]\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        one_hot = torch.zeros(len(true[key]), n_classes).scatter_(1, torch.tensor(np.array(true[key]).reshape(len(np.array(true[key])), 1)), 1)\n",
    "        for i in range(n_classes):\n",
    "            # fpr[i], tpr[i], _ = roc_curve(one_hot[:, i], np.array(pre[key])[:, i])\n",
    "            fpr[i], tpr[i], _ = roc_curve(one_hot[:, i], np.array(nn.Softmax(dim=1)(torch.Tensor(pre[key])))[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(np.array(one_hot).ravel(), np.array(nn.Softmax(dim=1)(torch.Tensor(pre[key]))).ravel())\n",
    "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "        # Then interpolate all ROC curves at this points\n",
    "        mean_tpr = np.zeros_like(all_fpr)\n",
    "        for i in range(n_classes):\n",
    "            mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "        # Finally average it and compute AUC\n",
    "        mean_tpr /= n_classes\n",
    "        fpr[\"macro\"] = all_fpr\n",
    "        tpr[\"macro\"] = mean_tpr\n",
    "        roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "        \n",
    "        if show == True and key =='diag':\n",
    "            # Plot all ROC curves\n",
    "            color_list = ['aqua', 'darkorange', 'cornflowerblue', 'green', 'brown']\n",
    "            lw=2\n",
    "            plt.figure()\n",
    "\n",
    "            colors = cycle(color_list[0:n_classes])\n",
    "            \n",
    "            name_diag=['BCC','NEV','MEL','MISC','SK']\n",
    "            \n",
    "            for i, color in zip(range(n_classes), colors):\n",
    "                plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "                         label='ROC curve of class {0} (area = {1:0.1f})'\n",
    "                         ''.format(name_diag[i], roc_auc[i] * 100))\n",
    "\n",
    "            plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            # plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "            plt.legend(loc=\"lower right\", prop=font1)\n",
    "            plt.savefig('./visualization/ROC.pdf')\n",
    "            plt.show()\n",
    "        auc_all[key] = roc_auc\n",
    "    return auc_all\n",
    "\n",
    "def cal_con_matrix(pre, true):\n",
    "    acc_all = {}\n",
    "    con_all = {}\n",
    "    tp_fp = {}\n",
    "    for key in pre.keys():\n",
    "        acc = accuracy_score(np.array(true[key]), np.argmax(np.array(pre[key]), axis=-1))\n",
    "        con = confusion_matrix(np.array(true[key]), np.argmax(np.array(pre[key]), axis=-1))\n",
    "        acc_all[key] = acc\n",
    "        con_all[key] = con\n",
    "    return acc_all, con_all\n",
    "\n",
    "def metric(pre, true, show = False):\n",
    "    auc_all = cal_auc(pre=pre, true=true, show = show)\n",
    "    acc_all, con_all = cal_con_matrix(pre=pre, true=true)\n",
    "    return auc_all, acc_all, con_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "featured-algeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")# (\"cuda:0\")\n",
    "class CNN(nn.Module): \n",
    "    def __init__(self, model):\n",
    "        super(CNN, self).__init__()\n",
    "        self.resnet_layer = nn.Sequential(*list(model.children())[:-2])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.resnet_layer(x)\n",
    "        return x\n",
    "class Concate(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(Concate, self).__init__()\n",
    "        self.hidden_size = 512 # 512\n",
    "        \n",
    "        self.avp_pooling = nn.AdaptiveAvgPool2d((1, 1)) # AdaptiveAvgPool2d\n",
    "        self.linear_layer1 = nn.Linear(2048 * 2, self.hidden_size) # reduce the dimensional\n",
    "        \n",
    "        # attention computation using SEblock\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2048, 2048 // 2, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(2048 // 2, 2048, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.att_relu = nn.ReLU()\n",
    "        \n",
    "        # define classifiers\n",
    "        self.out_diag = nn.Linear(self.hidden_size, 5)\n",
    "        self.out_crit_pn = nn.Linear(self.hidden_size, 3) # p_n 3, s_t_r 3, p_i_g 3, r_s 2, d_a_g 3, b_w_v 2, v_s 3\n",
    "        self.out_crit_str = nn.Linear(self.hidden_size, 3)\n",
    "        self.out_crit_pig = nn.Linear(self.hidden_size, 3)\n",
    "        self.out_crit_rs = nn.Linear(self.hidden_size, 2)\n",
    "        self.out_crit_dag = nn.Linear(self.hidden_size, 3)\n",
    "        self.out_crit_bwv = nn.Linear(self.hidden_size, 2)\n",
    "        self.out_crit_vs = nn.Linear(self.hidden_size, 3)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        \n",
    "    def forward(self, x_c, x_d):\n",
    "        b, c, _, _ = x_c.size()\n",
    "        img_size = torch.rand(x_c.size()[0], 3, x_c.size()[2], x_c.size()[3])\n",
    "        # SE feature compute\n",
    "        x_att_c = self.avg_pool(x_c).view(b, c)\n",
    "        x_att_c = self.fc(x_att_c).view(b, c, 1, 1)\n",
    "        x_att_feature_c = x_c * x_att_c.expand_as(x_c)\n",
    "        \n",
    "        x_att_d = self.avg_pool(x_d).view(b, c)\n",
    "        x_att_d = self.fc(x_att_d).view(b, c, 1, 1)\n",
    "        x_att_feature_d = x_d * x_att_d.expand_as(x_d)\n",
    "        \n",
    "        x_concat = torch.cat((x_c, x_d), dim=1)# x_att_feature_c, x_att_feature_d\n",
    "         \n",
    "        x_att_mask_c = torch.sum(x_c, dim=1) # x_att_feature_c\n",
    "        x_att_mask_d = torch.sum(x_d, dim=1)\n",
    "        # x_att_mask_c = x_att_c\n",
    "        # x_att_mask_d = x_att_d\n",
    "        x_att_mask_c = (x_att_mask_c - x_att_mask_c.min())/(x_att_mask_c.max() - x_att_mask_c.min()) #  (x - X_min) / (X_max - X_min)\n",
    "        x_att_mask_d = (x_att_mask_d - x_att_mask_d.min())/(x_att_mask_d.max() - x_att_mask_d.min())\n",
    "        # x_att_mask_c = torch.sum(x_att_feature_c, dim=1) / torch.sum(x_att_feature_c, dim=1).max()\n",
    "        # x_att_mask_d = torch.sum(x_att_feature_d, dim=1) / torch.sum(x_att_feature_d, dim=1).max()\n",
    "        x_att_mask_c = nn.functional.interpolate(x_att_mask_c.view(x_c.size()[0], 1, x_c.size()[2], x_c.size()[3]), size=(299, 299), mode='bilinear', align_corners=False) # bicubic, align_corners=False\n",
    "        x_att_mask_d = nn.functional.interpolate(x_att_mask_d.view(x_d.size()[0], 1, x_d.size()[2], x_d.size()[3]), size=(299, 299), mode='bilinear', align_corners=False) # bicubic, align_corners=False\n",
    "        \n",
    "        # flatten feature vectors\n",
    "        x = self.avp_pooling(x_concat).view(x_c.size()[0], -1)\n",
    "        x = torch.relu(self.linear_layer1(x))\n",
    "        x = self.dropout(x)\n",
    "        # classifiers\n",
    "        x_diag = self.out_diag(x)\n",
    "        x_crit_pn = self.out_crit_pn(x)\n",
    "        x_crit_str = self.out_crit_str(x)\n",
    "        x_crit_pig = self.out_crit_pig(x)\n",
    "        x_crit_rs = self.out_crit_rs(x)\n",
    "        x_crit_dag = self.out_crit_dag(x)\n",
    "        x_crit_bwv = self.out_crit_bwv(x)\n",
    "        x_crit_vs = self.out_crit_vs(x)\n",
    "        return x_diag, x_crit_pn, x_crit_str, x_crit_pig, x_crit_rs, x_crit_dag, x_crit_bwv, x_crit_vs, x_att_mask_c, x_att_mask_d, x_concat\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.hidden_size = 256\n",
    "        self.grl = WarmStartGradientReverseLayer(alpha=1., lo=0., hi=1., max_iters=100, auto_step=True) \n",
    "        self.hidden_layer = nn.Linear(2048, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, 2)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x_c, x_d):\n",
    "        x = torch.cat((x_c, x_d), dim = 0)\n",
    "        x = self.avg_pool(x).view(x.size()[0], -1)\n",
    "        x = self.grl(x)\n",
    "        x = self.relu(self.hidden_layer(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "resnet501 = models.resnet50(pretrained=True)\n",
    "cnn_c = CNN(resnet50).to(device)\n",
    "cnn_d = CNN(resnet501).to(device)\n",
    "concate_net = Concate().to(device)\n",
    "discriminator = Discriminator().to(device)# 判别分布"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-buyer",
   "metadata": {},
   "source": [
    "# Attention based reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dated-premium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv2d') != -1:\n",
    "        nn.init.kaiming_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.01)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight, 0.0, 0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.01)\n",
    "class ReconstructionNet(nn.Module):\n",
    "    def __init__(self, in_feature, output_size):\n",
    "        super(ReconstructionNet, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.up1 = nn.Sequential(\n",
    "                               nn.Upsample(scale_factor=4, mode='bicubic', align_corners=True),\n",
    "                               nn.Conv2d(in_feature, 128, 3, bias=False),\n",
    "                               nn.BatchNorm2d(128),\n",
    "                               nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.up2 = nn.Sequential(\n",
    "                               nn.Upsample(scale_factor=4, mode='bicubic', align_corners=True),\n",
    "                               nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                               nn.BatchNorm2d(64),\n",
    "                               nn.LeakyReLU(0.2),\n",
    "\n",
    "        )\n",
    "        self.up3 = nn.Sequential(\n",
    "                               nn.Upsample(scale_factor=2, mode='bicubic', align_corners=True),\n",
    "                               nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                               nn.BatchNorm2d(32),\n",
    "                               nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.Sigmoid = nn.Sigmoid()#nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.final_conv =nn.Conv2d(32, 3, 3, padding=1) # , padding=1\n",
    "        # self.final_conv1 =nn.Conv2d(32, 3, kernel_size = 1) # , padding=1\n",
    "        self.seg_layers = nn.Sequential(self.up1, self.up2, self.up3)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.seg_layers(x)\n",
    "        x = nn.functional.interpolate(x, size=self.output_size, mode='bicubic', align_corners=True)\n",
    "        x = self.final_conv(x)\n",
    "        # x = self.final_conv1(x)\n",
    "        y = self.Sigmoid(x)\n",
    "        # y = (torch.sin(x) + 1)/2.\n",
    "        return y\n",
    "\n",
    "    def get_parameters(self):\n",
    "        parameter_list = [{\"params\":self.parameters(), \"lr_mult\":1, 'decay_mult':2}]\n",
    "        return parameter_list\n",
    "\n",
    "'''def reconstruction_loss(pred=None, ground_truth=None, mask=None, crit=None):\n",
    "    pred1 = pred.view(pred.size()[0], -1)\n",
    "    ground_truth1 = pred.view(ground_truth.size()[0], -1)\n",
    "    \n",
    "    loss = crit(pred.view(pred.size()[0], -1),  ground_truth.view(ground_truth.size()[0], -1))\n",
    "    return loss '''\n",
    "def reconstruction_loss(pred=None, ground_truth=None, mask=None, crit=None):\n",
    "    '''import pdb;\n",
    "    pdb.set_trace()'''\n",
    "    pred1 = pred.view(pred.size()[0], -1)\n",
    "    ground_truth1 = pred.view(ground_truth.size()[0], -1)\n",
    "    \n",
    "    loss = crit(pred.view(pred.size()[0], -1),  ground_truth.view(ground_truth.size()[0], -1))\n",
    "    if mask != None:\n",
    "        mask1 = torch.cat((mask, mask, mask), 1)\n",
    "        mask1 = mask1.view(pred.size()[0], -1).detach()\n",
    "        # weighted loss \n",
    "        loss = loss * torch.exp(mask1) # torch.exp()\n",
    "    loss = torch.mean(torch.sum(loss, dim=1) / loss.size()[1]) # sum or mean\n",
    "    return loss \n",
    "def show_reconstruction_batch(batch_img, mask = False): # show orignal images, attention maps and reconstruction images in one batch\n",
    "    if mask == False:\n",
    "        grid_img = torchvision.utils.make_grid(batch_img, nrow=4)\n",
    "        plt.imshow(grid_img.permute(1, 2, 0).squeeze())\n",
    "        plt.show()\n",
    "    elif mask == True:\n",
    "        # import pdb;pdb.set_trace() # torch.Size([13, 1, 299, 299])\n",
    "        grid_img = torchvision.utils.make_grid(batch_img, nrow=4)\n",
    "        plt.imshow(grid_img.permute(1, 2, 0).squeeze())\n",
    "        plt.show()\n",
    "    \n",
    "reconstruct_net_c = ReconstructionNet(in_feature=2048 * 2, output_size=(299, 299)).to(device)\n",
    "reconstruct_net_d = ReconstructionNet(in_feature=2048 * 2, output_size=(299, 299)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "international-typing",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "learning_rate_re = 1e-5\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt_list = chain(cnn_c.parameters(), cnn_d.parameters(), concate_net.parameters(), reconstruct_net_c.parameters(), reconstruct_net_d.parameters(), discriminator.parameters())\n",
    "# optimizer = optim.Adam(chain(reconstruct_net_c.parameters(), reconstruct_net_d.parameters(), concate_net.parameters(), cnn_c.parameters(), cnn_d.parameters()), lr=learning_rate , weight_decay=0.0001) #\n",
    "optimizer = optim.Adam(opt_list, lr=learning_rate, weight_decay=0.0001) # , weight_decay=0.0001\n",
    "# optimizer_con = optim.Adam(chain(concate_net.parameters(), discriminator.parameters()), lr=learning_rate) # , weight_decay=0.0001\n",
    "# optimizer_re = optim.Adam(chain(reconstruct_net_c.parameters(), reconstruct_net_d.parameters()), lr=learning_rate_re) # , weight_decay=0.0001\n",
    "# criterion_recon = nn.MSELoss()\n",
    "criterion_recon = nn.MSELoss(reduction='none')\n",
    "criterion_l1 = nn.L1Loss(reduction='none')\n",
    "def generate_label(batch_size):\n",
    "    l_c = torch.zeros(batch_size)\n",
    "    l_d = torch.ones(batch_size)\n",
    "    label = torch.cat((l_c, l_d), dim=0)\n",
    "    return label\n",
    "\n",
    "def test_fun(dataloader, model_c, model_d, model_concate, model_recon_c, model_recon_d, epoch):\n",
    "    model_c.eval()\n",
    "    model_d.eval()\n",
    "    model_concate.eval()\n",
    "    loss_diag_count = []\n",
    "    loss_crit_pn_count = []\n",
    "    loss_crit_str_count = []\n",
    "    loss_crit_pig_count = []\n",
    "    loss_crit_rs_count = []\n",
    "    loss_crit_dag_count = []\n",
    "    loss_crit_bwv_count = []\n",
    "    loss_crit_vs_count = []\n",
    "    label_true = {'diag':[], 'pn':[], 'str':[], 'pig':[], 'rs':[], 'dag':[], 'bwv':[], 'vs':[]}\n",
    "    pred_all = {'diag':[], 'pn':[], 'str':[], 'pig':[], 'rs':[], 'dag':[], 'bwv':[], 'vs':[]}\n",
    "    with torch.no_grad():\n",
    "        for index, data in enumerate(dataloader):\n",
    "            img_c, img_d, lab_diag, lab_crit = data\n",
    "            label_true['diag'].extend(lab_diag)\n",
    "            img_c, img_d, lab_diag = img_c.to(device), img_d.to(device), lab_diag.to(device)\n",
    "            # print(img_c.shape)\n",
    "            # import pdb;pdb.set_trace()\n",
    "            label_true['pn'].extend(lab_crit[:, 0]), \\\n",
    "                        label_true['str'].extend(lab_crit[:, 1]), label_true['pig'].extend(lab_crit[:, 2]), label_true['rs'].extend(lab_crit[:, 3]), \\\n",
    "                        label_true['dag'].extend(lab_crit[:, 4]), label_true['bwv'].extend(lab_crit[:, 5]), label_true['vs'].extend(lab_crit[:, 6])\n",
    "            lab_crit_pn, lab_crit_str, lab_crit_pig, lab_crit_rs, lab_crit_dag, lab_crit_bwv, lab_crit_vs = lab_crit[:, 0].to(device), lab_crit[:, 1].to(device), lab_crit[:, 2].to(device), lab_crit[:, 3].to(device), lab_crit[:, 4].to(device), lab_crit[:, 5].to(device), lab_crit[:, 6].to(device)\n",
    "\n",
    "            # print(lab_crit_dag)\n",
    "            feature_c = model_c(img_c)# predict for each class\n",
    "            feature_d = model_d(img_d)# predict for each class\n",
    "            prediction = model_concate(feature_c, feature_d)\n",
    "            out_diag, out_crit_pn, out_crit_str, out_crit_pig, out_crit_rs, out_crit_dag, out_crit_bwv, out_crit_vs, \\\n",
    "                        att_mask_c, att_mask_d, att_feature = prediction\n",
    "            \n",
    "            recon_pred_c = model_recon_c(att_feature)\n",
    "            recon_pred_d = model_recon_d(att_feature)\n",
    "\n",
    "            pred_all['diag'].extend(out_diag.cpu().detach().numpy()), pred_all['pn'].extend(out_crit_pn.cpu().detach().numpy()), \\\n",
    "                            pred_all['str'].extend(out_crit_str.cpu().detach().numpy()), pred_all['pig'].extend(out_crit_pig.cpu().detach().numpy()), pred_all['rs'].extend(out_crit_rs.cpu().detach().numpy()), \\\n",
    "                            pred_all['dag'].extend(out_crit_dag.cpu().detach().numpy()), pred_all['bwv'].extend(out_crit_bwv.cpu().detach().numpy()), pred_all['vs'].extend(out_crit_vs.cpu().detach().numpy())\n",
    "            loss_diag = criterion(out_diag, lab_diag)\n",
    "            loss_crit_pn = criterion(out_crit_pn, lab_crit_pn)\n",
    "            loss_crit_str = criterion(out_crit_str, lab_crit_str)\n",
    "            loss_crit_pig = criterion(out_crit_pig, lab_crit_pig)\n",
    "            loss_crit_rs = criterion(out_crit_rs, lab_crit_rs)\n",
    "            loss_crit_dag = criterion(out_crit_dag, lab_crit_dag)\n",
    "            loss_crit_bwv = criterion(out_crit_bwv, lab_crit_bwv)\n",
    "            loss_crit_vs = criterion(out_crit_vs, lab_crit_vs)\n",
    "            loss_diag_count.append(loss_diag.item())\n",
    "            loss_crit_pn_count.append(loss_crit_pn.item())\n",
    "            loss_crit_str_count.append(loss_crit_str.item())\n",
    "            loss_crit_pig_count.append(loss_crit_pig.item())\n",
    "            loss_crit_rs_count.append(loss_crit_rs.item())\n",
    "            loss_crit_dag_count.append(loss_crit_dag.item())\n",
    "            loss_crit_bwv_count.append(loss_crit_bwv.item())\n",
    "            loss_crit_vs_count.append(loss_crit_vs.item())\n",
    "        print(\"Epoch: {} test loss, Diag loss: {:.4f}, PN loss: {:.4f}, STR loss: {:.4f}, PIG loss: {:.4f}, RS loss: {:.4f}, DaG loss: {:.4f}, BWV loss: {:.4f}, VS loss: {:.4f}\".format(\n",
    "                        epoch, np.average(loss_diag_count), np.average(loss_crit_pn_count), np.average(loss_crit_str_count), np.average(loss_crit_pig_count), np.average(loss_crit_rs_count), np.average(loss_crit_dag_count), np.average(loss_crit_bwv_count), np.average(loss_crit_vs_count)))\n",
    "    return pred_all, label_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "continent-breath",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_average_acc(acc):\n",
    "    accs = []\n",
    "    for key in acc.keys():\n",
    "        accs.append(acc[key])\n",
    "    avg_acc = np.average(accs)\n",
    "    return avg_acc\n",
    "def get_average_auc(auc):\n",
    "    aucs = []\n",
    "    for key in auc.keys():\n",
    "        if key == 'diag':\n",
    "            continue\n",
    "        else:\n",
    "            for key_i in auc[key].keys():\n",
    "                if key_i == 'micro' or key_i == 'macro':\n",
    "                    continue\n",
    "                else:\n",
    "                    aucs.append(auc[key][key_i])\n",
    "    # print(len(aucs))\n",
    "    avg_auc = np.average(aucs)\n",
    "    return avg_auc\n",
    "def get_specificity(pre, true):\n",
    "    sen = {}\n",
    "    for key in pre:\n",
    "        sen[key] = specificity_score(np.array(pre[key]).argmax(axis=1), true[key], average=None)\n",
    "    return sen\n",
    "def get_confusion_matrix(pre, true): # recall and precision\n",
    "    confusion_metric = {}\n",
    "    for key in pre:\n",
    "        # import pdb;pdb.set_trace()\n",
    "        confusion_metric[key] = classification_report(np.array(pre[key]).argmax(axis=1), true[key], zero_division  = 1, output_dict=True)\n",
    "    return confusion_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5e096fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 begin train...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 train loss, Diag loss: 1.2580, PN loss: 0.6311, STR loss: 0.5189, PIG loss: 0.4011, RS loss: 0.2221, DaG loss: 0.6555, BWV loss: 0.2774, VS loss: 0.3361\n",
      "0.8132911392405063 0.9372441951816893\n",
      "Test metics on epoch 1:\n",
      "{'diag': {0: 0.6368733509234829, 1: 0.5922322540473226, 2: 0.5901192160032329, 3: 0.6863380281690141, 4: 0.5431131019036954, 'micro': 0.8056369171607114, 'macro': 0.6114560493097589}, 'pn': {0: 0.9071183349426029, 1: 0.9264455080596358, 2: 0.9157231360820337, 'micro': 0.9149270950168242, 'macro': 0.9182676287829574}, 'str': {0: 0.9149326115152541, 1: 0.8748381248381248, 2: 0.9193822011733936, 'micro': 0.9319179618650857, 'macro': 0.9058376981152738}, 'pig': {0: 0.9778131191990823, 1: 0.9852305475504323, 2: 0.970479704797048, 'micro': 0.9741547828873578, 'macro': 0.9789697450152546}, 'rs': {0: 0.9865508911666775, 1: 0.9865508911666775, 'micro': 0.9861688831917963, 'macro': 0.9875009683070903}, 'dag': {0: 0.8962033898305084, 1: 0.9204246466377042, 2: 0.9036438086352563, 'micro': 0.904864605031245, 'macro': 0.9085776174966108}, 'bwv': {0: 0.9144166666666667, 1: 0.9144166666666667, 'micro': 0.952469155584041, 'macro': 0.9182476236979166}, 'vs': {0: 0.9816488739967272, 1: 0.9743776631531734, 2: 0.9374429223744293, 'micro': 0.976539016183304, 'macro': 0.965800047198841}}\n",
      "{'diag': 0.549367088607595, 'pn': 0.7670886075949367, 'str': 0.8, 'pig': 0.8886075949367088, 'rs': 0.9493670886075949, 'dag': 0.7822784810126582, 'bwv': 0.8987341772151899, 'vs': 0.8708860759493671}\n",
      "Current best average ACC: 0.8133\n",
      "Current average AUC: 0.9372\n",
      "\n",
      "Epoch 1 begin train...\n",
      "Epoch: 1 train loss, Diag loss: 1.1737, PN loss: 0.5960, STR loss: 0.4916, PIG loss: 0.3779, RS loss: 0.2173, DaG loss: 0.6192, BWV loss: 0.2516, VS loss: 0.3265\n",
      "0.8300632911392405 0.9442196638837924\n",
      "Test metics on epoch 1:\n",
      "{'diag': {0: 0.6226912928759895, 1: 0.6371938563719386, 2: 0.6395904896612109, 3: 0.7095070422535211, 4: 0.6858902575587905, 'micro': 0.8274811728889602, 'macro': 0.660878347022732}, 'pn': {0: 0.9058040982727175, 1: 0.9370633217802717, 2: 0.9279000213629567, 'micro': 0.9255760294824547, 'macro': 0.9253808567006747}, 'str': {0: 0.9170191169006936, 1: 0.8981481481481483, 2: 0.9356047218491553, 'micro': 0.9421791379586605, 'macro': 0.9195028145434576}, 'pig': {0: 0.9787777661904264, 1: 0.9912343900096061, 2: 0.9689025116057614, 'micro': 0.9803460983816695, 'macro': 0.9811555729925477}, 'rs': {0: 0.9845269961480708, 1: 0.9845269961480707, 'micro': 0.9848485819580195, 'macro': 0.9859079363774738}, 'dag': {0: 0.8994915254237288, 1: 0.9190479104203635, 2: 0.9198932255222102, 'micro': 0.9158852747957058, 'macro': 0.9145685680281566}, 'bwv': {0: 0.9345416666666666, 1: 0.9345416666666667, 'micro': 0.9643518666880306, 'macro': 0.9380282335069444}, 'vs': {0: 0.9905711836671083, 1: 0.9747701278313523, 2: 0.9378082191780822, 'micro': 0.9785451049511297, 'macro': 0.9710498479072222}}\n",
      "{'diag': 0.5721518987341773, 'pn': 0.8151898734177215, 'str': 0.810126582278481, 'pig': 0.9189873417721519, 'rs': 0.9341772151898734, 'dag': 0.8050632911392405, 'bwv': 0.9139240506329114, 'vs': 0.8708860759493671}\n",
      "Current best average ACC: 0.8301\n",
      "Current average AUC: 0.9442\n",
      "\n",
      "Epoch 2 begin train...\n",
      "Epoch: 2 train loss, Diag loss: 1.2032, PN loss: 0.5823, STR loss: 0.4718, PIG loss: 0.3438, RS loss: 0.1929, DaG loss: 0.5877, BWV loss: 0.2650, VS loss: 0.2944\n",
      "0.8420886075949368 0.9526269066482363\n",
      "Test metics on epoch 1:\n",
      "{'diag': {0: 0.691787598944591, 1: 0.6086031548360316, 2: 0.5687681012999259, 3: 0.7215492957746478, 4: 0.6947088465845465, 'micro': 0.8217208780644129, 'macro': 0.6590644395379344}, 'pn': {0: 0.9166666666666667, 1: 0.9533476371238377, 2: 0.9329915260272021, 'micro': 0.9353084441595899, 'macro': 0.9360257820043808}, 'str': {0: 0.9368127220436475, 1: 0.8870111370111371, 2: 0.9520039584364177, 'micro': 0.9497836885114564, 'macro': 0.9282888081975245}, 'pig': {0: 0.9944989049953071, 1: 0.9939361191162345, 2: 0.9837816926556362, 'micro': 0.9891748117288895, 'macro': 0.9916914626446314}, 'rs': {0: 0.9921329242018672, 1: 0.9921329242018672, 'micro': 0.9926422047748759, 'macro': 0.9928745854154883}, 'dag': {0: 0.9088813559322034, 1: 0.9346203267453955, 2: 0.9409371274555539, 'micro': 0.9322544464028202, 'macro': 0.9297558933740638}, 'bwv': {0: 0.9188333333333334, 1: 0.9188333333333334, 'micro': 0.9590193879186026, 'macro': 0.9224907660590278}, 'vs': {0: 0.9918179692979039, 1: 0.9849181430814085, 2: 0.9657534246575342, 'micro': 0.9842044544143567, 'macro': 0.9821206402294065}}\n",
      "{'diag': 0.5569620253164557, 'pn': 0.810126582278481, 'str': 0.8430379746835444, 'pig': 0.9316455696202531, 'rs': 0.9645569620253165, 'dag': 0.8202531645569621, 'bwv': 0.9240506329113924, 'vs': 0.8860759493670886}\n",
      "Current best average ACC: 0.8421\n",
      "Current average AUC: 0.9526\n",
      "\n",
      "Epoch 3 begin train...\n",
      "Epoch: 3 train loss, Diag loss: 1.1769, PN loss: 0.5525, STR loss: 0.4394, PIG loss: 0.3306, RS loss: 0.1827, DaG loss: 0.5632, BWV loss: 0.2512, VS loss: 0.2924\n",
      "0.8427215189873418 0.9608861119017327\n",
      "Test metics on epoch 1:\n",
      "{'diag': {0: 0.5972955145118732, 1: 0.6257783312577833, 2: 0.6136593251161853, 3: 0.7168309859154929, 4: 0.6328387458006719, 'micro': 0.8260471078352828, 'macro': 0.6391100282490457}, 'pn': {0: 0.9298626756785753, 1: 0.9535952027287231, 2: 0.946022929573453, 'micro': 0.9450440634513699, 'macro': 0.9447450051718366}, 'str': {0: 0.9491343822252298, 1: 0.9200984200984201, 2: 0.9608044108291511, 'micro': 0.9589264540938951, 'macro': 0.9461200001510222}, 'pig': {0: 0.9939253311085618, 1: 0.9911743515850144, 2: 0.9883942387810976, 'micro': 0.9916423650056081, 'macro': 0.9917881789120895}, 'rs': {0: 0.9947444016452308, 1: 0.9947444016452307, 'micro': 0.9939945521551034, 'macro': 0.9957320056137814}, 'dag': {0: 0.9185423728813559, 1: 0.9468579820106467, 2: 0.946820090188151, 'micro': 0.9400608876782567, 'macro': 0.9392933633147111}, 'bwv': {0: 0.9426666666666667, 1: 0.9426666666666667, 'micro': 0.968146130427816, 'macro': 0.9464837565104166}, 'vs': {0: 0.9932206031325488, 1: 0.9854788069073783, 2: 0.958082191780822, 'micro': 0.9836372376221758, 'macro': 0.9801171918856957}}\n",
      "{'diag': 0.5443037974683544, 'pn': 0.8455696202531645, 'str': 0.8379746835443038, 'pig': 0.9341772151898734, 'rs': 0.9670886075949368, 'dag': 0.8126582278481013, 'bwv': 0.9139240506329114, 'vs': 0.8860759493670886}\n",
      "Current best average ACC: 0.8427\n",
      "Current average AUC: 0.9609\n",
      "\n",
      "Epoch 4 begin train...\n",
      "Epoch: 4 train loss, Diag loss: 1.2025, PN loss: 0.5392, STR loss: 0.4377, PIG loss: 0.3037, RS loss: 0.1838, DaG loss: 0.5230, BWV loss: 0.2497, VS loss: 0.3112\n",
      "0.8515822784810125 0.9601778784245564\n",
      "Test metics on epoch 1:\n",
      "{'diag': {0: 0.6241754617414248, 1: 0.6073318804483188, 2: 0.5952380952380952, 3: 0.7161267605633803, 4: 0.6991881298992161, 'micro': 0.8210334882230412, 'macro': 0.6502650234857589}, 'pn': {0: 0.9391427958373566, 1: 0.9592892116410849, 2: 0.9385458947518337, 'micro': 0.9473577952251241, 'macro': 0.9473408721876079}, 'str': {0: 0.9449049794169063, 1: 0.9155011655011654, 2: 0.9536297448222238, 'micro': 0.9583624419163594, 'macro': 0.9402019499953981}, 'pig': {0: 0.997966419856085, 1: 0.9973583093179635, 2: 0.9937507439590525, 'micro': 0.9950745072904983, 'macro': 0.9968373316355623}, 'rs': {0: 0.9949729059215251, 1: 0.9949729059215251, 'micro': 0.9936869091491748, 'macro': 0.9956267726802275}, 'dag': {0: 0.9301016949152542, 1: 0.9665912011258643, 2: 0.9560980666563001, 'micro': 0.9529947123858356, 'macro': 0.9526982465287313}, 'bwv': {0: 0.9405000000000001, 1: 0.9405000000000001, 'micro': 0.9675116167280884, 'macro': 0.9432780490451389}, 'vs': {0: 0.9840645211563936, 1: 0.9678178963893248, 2: 0.9276712328767123, 'micro': 0.9788239064252524, 'macro': 0.963578845992081}}\n",
      "{'diag': 0.569620253164557, 'pn': 0.8405063291139241, 'str': 0.850632911392405, 'pig': 0.9518987341772152, 'rs': 0.9518987341772152, 'dag': 0.8607594936708861, 'bwv': 0.9113924050632911, 'vs': 0.8759493670886076}\n",
      "Current best average ACC: 0.8516\n",
      "Current average AUC: 0.9602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_fun(dataloader, model_c, model_d, model_concate, model_recon_c, model_recon_d, optimizer, epoch):\n",
    "    model_c.train()\n",
    "    model_d.train()\n",
    "    model_concate.train()\n",
    "    loss_diag_count = []\n",
    "    loss_crit_pn_count = []\n",
    "    loss_crit_str_count = []\n",
    "    loss_crit_pig_count = []\n",
    "    loss_crit_rs_count = []\n",
    "    loss_crit_dag_count = []\n",
    "    loss_crit_bwv_count = []\n",
    "    loss_crit_vs_count = []\n",
    "    label_true = {'diag':[], 'pn':[], 'str':[], 'pig':[], 'rs':[], 'dag':[], 'bwv':[], 'vs':[]}\n",
    "    pred_all = {'diag':[], 'pn':[], 'str':[], 'pig':[], 'rs':[], 'dag':[], 'bwv':[], 'vs':[]}\n",
    "    for index, data in enumerate(dataloader):\n",
    "        img_c, img_d, lab_diag, lab_crit = data\n",
    "        label_true['diag'].extend(lab_diag)\n",
    "        img_c, img_d, lab_diag = img_c.to(device), img_d.to(device), lab_diag.to(device)\n",
    "        # print(img_c.shape)\n",
    "        # import pdb;pdb.set_trace()\n",
    "        label_true['pn'].extend(lab_crit[:, 0]), \\\n",
    "                    label_true['str'].extend(lab_crit[:, 1]), label_true['pig'].extend(lab_crit[:, 2]), label_true['rs'].extend(lab_crit[:, 3]), \\\n",
    "                    label_true['dag'].extend(lab_crit[:, 4]), label_true['bwv'].extend(lab_crit[:, 5]), label_true['vs'].extend(lab_crit[:, 6])\n",
    "        lab_crit_pn, lab_crit_str, lab_crit_pig, lab_crit_rs, lab_crit_dag, lab_crit_bwv, lab_crit_vs = lab_crit[:, 0].to(device), lab_crit[:, 1].to(device), lab_crit[:, 2].to(device), lab_crit[:, 3].to(device), lab_crit[:, 4].to(device), lab_crit[:, 5].to(device), lab_crit[:, 6].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # print(lab_crit_dag)\n",
    "        feature_c = model_c(img_c)# predict for each class\n",
    "        feature_d = model_d(img_d)# predict for each class\n",
    "        prediction = model_concate(feature_c, feature_d)\n",
    "        out_diag, out_crit_pn, out_crit_str, out_crit_pig, out_crit_rs, out_crit_dag, out_crit_bwv, out_crit_vs, \\\n",
    "                    att_mask_c, att_mask_d, att_feature = prediction\n",
    "        \n",
    "        recon_pred_c = model_recon_c(att_feature)\n",
    "        recon_pred_d = model_recon_d(att_feature)\n",
    "\n",
    "        pred_all['diag'].extend(out_diag.cpu().detach().numpy()), pred_all['pn'].extend(out_crit_pn.cpu().detach().numpy()), \\\n",
    "                        pred_all['str'].extend(out_crit_str.cpu().detach().numpy()), pred_all['pig'].extend(out_crit_pig.cpu().detach().numpy()), pred_all['rs'].extend(out_crit_rs.cpu().detach().numpy()), \\\n",
    "                        pred_all['dag'].extend(out_crit_dag.cpu().detach().numpy()), pred_all['bwv'].extend(out_crit_bwv.cpu().detach().numpy()), pred_all['vs'].extend(out_crit_vs.cpu().detach().numpy())\n",
    "        loss_diag = criterion(out_diag, lab_diag)\n",
    "        loss_crit_pn = criterion(out_crit_pn, lab_crit_pn)\n",
    "        loss_crit_str = criterion(out_crit_str, lab_crit_str)\n",
    "        loss_crit_pig = criterion(out_crit_pig, lab_crit_pig)\n",
    "        loss_crit_rs = criterion(out_crit_rs, lab_crit_rs)\n",
    "        loss_crit_dag = criterion(out_crit_dag, lab_crit_dag)\n",
    "        loss_crit_bwv = criterion(out_crit_bwv, lab_crit_bwv)\n",
    "        loss_crit_vs = criterion(out_crit_vs, lab_crit_vs)\n",
    "        loss_diag_count.append(loss_diag.item())\n",
    "        loss_crit_pn_count.append(loss_crit_pn.item())\n",
    "        loss_crit_str_count.append(loss_crit_str.item())\n",
    "        loss_crit_pig_count.append(loss_crit_pig.item())\n",
    "        loss_crit_rs_count.append(loss_crit_rs.item())\n",
    "        loss_crit_dag_count.append(loss_crit_dag.item())\n",
    "        loss_crit_bwv_count.append(loss_crit_bwv.item())\n",
    "        loss_crit_vs_count.append(loss_crit_vs.item())\n",
    "\n",
    "        loss = loss_diag + loss_crit_pn + loss_crit_str + loss_crit_pig + loss_crit_rs + loss_crit_dag + loss_crit_bwv + loss_crit_vs\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch: {} train loss, Diag loss: {:.4f}, PN loss: {:.4f}, STR loss: {:.4f}, PIG loss: {:.4f}, RS loss: {:.4f}, DaG loss: {:.4f}, BWV loss: {:.4f}, VS loss: {:.4f}\".format(\n",
    "                    epoch, np.average(loss_diag_count), np.average(loss_crit_pn_count), np.average(loss_crit_str_count), np.average(loss_crit_pig_count), np.average(loss_crit_rs_count), np.average(loss_crit_dag_count), np.average(loss_crit_bwv_count), np.average(loss_crit_vs_count)))\n",
    "    return pred_all, label_true\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(test_dataset, batch_size=12, shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "from time import time\n",
    "total_time_train = 0\n",
    "for epoch in range(5):\n",
    "        # training\n",
    "        start_time_epoch_train = time()\n",
    "\n",
    "        print(\"Epoch {} begin train...\".format(epoch))\n",
    "        pred_all_train, label_true_train = train_fun(trainloader, cnn_c, cnn_d, concate_net, reconstruct_net_c, reconstruct_net_d, optimizer, epoch)\n",
    "        auc_all_train, acc_all_train, con_all_train = metric(pred_all_train, label_true_train, show=False)\n",
    "        avg_acc = get_average_acc(acc_all_train)# get the average acc\n",
    "        avg_auc = get_average_auc(auc_all_train)\n",
    "        con_metric = get_confusion_matrix(pred_all_train, label_true_train) # compute recall and precision\n",
    "        specificity = get_specificity(pred_all_train, label_true_train)\n",
    "        # sens, spec, prec = get_confusion_matrix(con_all_test)\n",
    "        print(avg_acc, avg_auc)\n",
    "        # if i % 10 == 0 or i == (epochs - 1):\n",
    "        if (record_acc+record_auc) <= (avg_acc + avg_auc):\n",
    "            record_acc = avg_acc\n",
    "            record_auc = avg_auc\n",
    "            print(\"Train metics on epoch {}:\".format(epoch))\n",
    "            print(auc_all_train)\n",
    "            print(acc_all_train)\n",
    "            print(\"Current best average ACC: {:.4f}\".format(avg_acc))\n",
    "            print(\"Current average AUC: {:.4f}\".format(avg_auc))\n",
    "            print()\n",
    "\n",
    "        end_time_epoch_train = time()\n",
    "        epoch_time_train = end_time_epoch_train - start_time_epoch_train\n",
    "        total_time_train += epoch_time_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bea0530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4b38f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7ed0fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bac7c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a759936d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 begin testing...\n",
      "Epoch: 1 test loss, Diag loss: 0.8220, PN loss: 0.6805, STR loss: 0.5907, PIG loss: 0.7437, RS loss: 0.3653, DaG loss: 0.8746, BWV loss: 0.2866, VS loss: 0.4405\n",
      "0.7591772151898735 0.8473569795321536\n",
      "Test metics on epoch 1:\n",
      "{'diag': {0: 0.941787598944591, 1: 0.8965338314653383, 2: 0.8903482184953189, 3: 0.9059859154929577, 4: 0.8171892497200448, 'micro': 0.9339593013940073, 'macro': 0.8917050790802882}, 'pn': {0: 0.892018023817187, 1: 0.844886394894647, 2: 0.8205155593534144, 'micro': 0.8611728889601025, 'macro': 0.8542553682972424}, 'str': {0: 0.8300343991428409, 1: 0.8954286454286454, 2: 0.8073443132819679, 'micro': 0.8978497035731454, 'macro': 0.846272765996983}, 'pig': {0: 0.8073573886745229, 1: 0.8515249759846302, 2: 0.8338590643970956, 'micro': 0.8701810607274474, 'macro': 0.8326071577107976}, 'rs': {0: 0.8675980936214664, 1: 0.8675980936214662, 'micro': 0.906771350745073, 'macro': 0.8702723924781556}, 'dag': {0: 0.7717966101694915, 1: 0.7768463562381448, 2: 0.8192090395480226, 'micro': 0.8018394488062811, 'macro': 0.7910392199148842}, 'bwv': {0: 0.9108333333333334, 1: 0.9108333333333333, 'micro': 0.9532703092453133, 'macro': 0.9132355794270833}, 'vs': {0: 0.8958154757266422, 1: 0.8876990356582193, 2: 0.8085844748858448, 'micro': 0.9475628905624098, 'macro': 0.8672252684427376}}\n",
      "{'diag': 0.7544303797468355, 'pn': 0.7037974683544304, 'str': 0.7468354430379747, 'pig': 0.7088607594936709, 'rs': 0.8075949367088607, 'dag': 0.6379746835443038, 'bwv': 0.8810126582278481, 'vs': 0.8329113924050633}\n",
      "Current best average ACC: 0.7592\n",
      "Current average AUC: 0.8474\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "record_acc = 0.\n",
    "record_auc = 0.\n",
    "\n",
    "model_name_c = './checkpoint/feature_extraction_c_fusion_9-12_21.pth'# './checkpoint/feature_extraction_concate_discrinimator_0713_c1_two_stream.pth' # 3 ahieved the best performance \n",
    "model_name_d = './checkpoint/feature_extraction_d_fusion_9-12_21.pth' #'./checkpoint/feature_extraction_concate_discrinimator_0713_d1_two_stream.pth'\n",
    "model_name_concate = './checkpoint/concatenate_fusion_9-12_21.pth'# './checkpoint/concate_discrinimator_0713_concatenate1_two_stream.pth'\n",
    "model_name_dis_c = './checkpoint/discriminator_fusion_9-12_21.pth'# './checkpoint/reconstruct_concate_discrinimator_0713_c1_two_stream.pth'\n",
    "model_name_recon_c = './checkpoint/reconstruct_c_fusion_9-12_21.pth'# './checkpoint/feature_extraction_concate_recon_0713_c1_two_stream.pth' # 3 ahieved the best performance\n",
    "model_name_recon_d = './checkpoint/reconstruct_c_fusion_9-12_21.pth'# './checkpoint/feature_extraction_concate_recon_0713_d1_two_stream.pth'\n",
    "\n",
    "checkpoint_c = torch.load(model_name_c)\n",
    "cnn_c.load_state_dict(checkpoint_c)\n",
    "checkpoint_d = torch.load(model_name_d)\n",
    "cnn_d.load_state_dict(checkpoint_d)\n",
    "checkpoint_concate_net = torch.load(model_name_concate)\n",
    "concate_net.load_state_dict(checkpoint_concate_net)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=48,\n",
    "                                              shuffle=False, num_workers=4)\n",
    "i=1\n",
    "print(\"Epoch {} begin testing...\".format(i))\n",
    "pred_all_test, label_true_test = test_fun(testloader, cnn_c, cnn_d, concate_net, reconstruct_net_c, reconstruct_net_d, i)\n",
    "auc_all_test, acc_all_test, con_all_test = metric(pred_all_test, label_true_test, show=False)\n",
    "avg_acc = get_average_acc(acc_all_test)# get the average acc\n",
    "avg_auc = get_average_auc(auc_all_test)\n",
    "con_metric = get_confusion_matrix(pred_all_test, label_true_test) # compute recall and precision\n",
    "specificity = get_specificity(pred_all_test, label_true_test)\n",
    "# sens, spec, prec = get_confusion_matrix(con_all_test)\n",
    "print(avg_acc, avg_auc)\n",
    "# if i % 10 == 0 or i == (epochs - 1):\n",
    "if (record_acc+record_auc) <= (avg_acc + avg_auc):\n",
    "    record_acc = avg_acc\n",
    "    record_auc = avg_auc\n",
    "    print(\"Test metics on epoch {}:\".format(i))\n",
    "    print(auc_all_test)\n",
    "    print(acc_all_test)\n",
    "    print(\"Current best average ACC: {:.4f}\".format(avg_acc))\n",
    "    print(\"Current average AUC: {:.4f}\".format(avg_auc))\n",
    "    # print('confusion_matrix' + str(con_metric))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55e9dde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
