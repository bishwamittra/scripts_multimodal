{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import socket\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from rescalenet.layers import AvgPool2d, Bias2DMean\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T03:23:04.941692Z",
     "start_time": "2023-06-12T03:23:03.944033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client starts from:  0\n",
      "Files already downloaded and verified\n",
      "Train Size (x, y): \n",
      "torch.Size([256, 3, 32, 32]) \n",
      " torch.Size([256])\n",
      "Total Batch Number\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "root_path = 'cifar10_data'\n",
    "\n",
    "# Setup cpu\n",
    "device = 'cpu'\n",
    "\n",
    "# Setup client order\n",
    "client_order = int(0)\n",
    "print('Client starts from: ', client_order)\n",
    "batch_size = 256\n",
    "num_train_data = 5000\n",
    "\n",
    "# Load data\n",
    "from random import shuffle\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "indices = list(range(50000))\n",
    "\n",
    "part_tr = indices[num_train_data * client_order : num_train_data * (client_order + 1)]\n",
    "\n",
    "train_set  = torchvision.datasets.CIFAR10(root=root_path, train=True, download=True, transform=transform)\n",
    "train_set_sub = Subset(train_set, part_tr)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set_sub, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
    "256\n",
    "x_train, y_train = next(iter(train_loader))\n",
    "print('Train Size (x, y): ')\n",
    "print(x_train.size(), '\\n', y_train.size())\n",
    "\n",
    "total_batch = len(train_loader)\n",
    "print('Total Batch Number')\n",
    "print(total_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for communication between client and server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T03:23:04.947320Z",
     "start_time": "2023-06-12T03:23:04.945496Z"
    }
   },
   "outputs": [],
   "source": [
    "def send_msg(sock, msg):\n",
    "    # prefix each message with a 4-byte length in network byte order\n",
    "    msg = pickle.dumps(msg)\n",
    "    msg = struct.pack('>I', len(msg)) + msg\n",
    "    sock.sendall(msg)\n",
    "\n",
    "def recv_msg(sock):\n",
    "    # read message length and unpack it into an integer\n",
    "    raw_msg_len = recv_all(sock, 4)\n",
    "    if not raw_msg_len:\n",
    "        return None\n",
    "    msg_len = struct.unpack('>I', raw_msg_len)[0]\n",
    "    # read the message data\n",
    "    msg =  recv_all(sock, msg_len)\n",
    "    msg = pickle.loads(msg)\n",
    "    return msg\n",
    "\n",
    "def recv_all(sock, n):\n",
    "    # helper function to receive n bytes or return None if EOF is hit\n",
    "    data = b''\n",
    "    while len(data) < n:\n",
    "        packet = sock.recv(n - len(data))\n",
    "        if not packet:\n",
    "            return None\n",
    "        data += packet\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of client side model (input layer only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T03:23:04.957925Z",
     "start_time": "2023-06-12T03:23:04.955930Z"
    }
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(self,\n",
    "                 inplanes: int,\n",
    "                 planes: int,\n",
    "                 block_idx: int,\n",
    "                 max_block: int,\n",
    "                 stride: int = 1,\n",
    "                 groups: int = 1,\n",
    "                 base_width: int = 64,\n",
    "                 drop_conv=0.0) -> None:\n",
    "\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, padding=1, stride=stride, groups=groups, bias=False)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, stride=1, groups=groups, bias=False)\n",
    "\n",
    "        self.addbias1 = Bias2DMean(inplanes)\n",
    "        self.addbias2 = Bias2DMean(planes)\n",
    "\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.stride = stride\n",
    "        self._scale = nn.Parameter(torch.ones(1))\n",
    "        multiplier = (block_idx + 1)**-(1 / 6) * max_block**(1 / 6)\n",
    "        multiplier = multiplier * (1 - drop_conv)**.5\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) and m.weight.requires_grad:\n",
    "                _, C, H, W = m.weight.shape\n",
    "                stddev = (C * H * W / 2)**-.5\n",
    "                nn.init.normal_(m.weight, std=stddev * multiplier)\n",
    "\n",
    "        self.residual = max_block**-.5\n",
    "        self.identity = block_idx**.5 / (block_idx + 1)**.5\n",
    "\n",
    "        self.downsample = nn.Sequential()\n",
    "        if stride != 1 or inplanes != self.expansion * planes:\n",
    "            if stride == 1:\n",
    "                avgpool = nn.Sequential()\n",
    "            else:\n",
    "                avgpool = nn.AvgPool2d(stride)\n",
    "\n",
    "            self.downsample = nn.Sequential(avgpool, Bias2DMean(num_features=inplanes),\n",
    "                                            nn.Conv2d(inplanes, self.expansion * planes, kernel_size=1, bias=False))\n",
    "\n",
    "            nn.init.kaiming_normal_(self.downsample[2].weight, a=1)\n",
    "\n",
    "        self.drop = nn.Sequential()\n",
    "        if drop_conv > 0.0:\n",
    "            self.drop = nn.Dropout2d(drop_conv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Not adding dropout here.\n",
    "        out = F.relu(self.drop(self.conv1(self.addbias1(x))))\n",
    "        out = self.drop(self.conv2(self.addbias2(out)))\n",
    "        out = out * self.residual * self._scale + self.identity * self.downsample(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "    def init_pass(self, x, count):\n",
    "        out = F.relu(self.drop(self.conv1(self.addbias1.init_pass(x, count))))\n",
    "        out = self.drop(self.conv2(self.addbias2.init_pass(out, count)))\n",
    "        out = out * self.residual * self._scale + self.identity * self.downsample(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, block_idx, max_block, stride=1, groups=1, base_width=64, drop_conv=0.0):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(width, width, kernel_size=3, padding=1, stride=stride, groups=groups, bias=False)\n",
    "        self.conv3 = nn.Conv2d(width, planes * self.expansion, kernel_size=1, bias=False)\n",
    "\n",
    "        self.addbias1 = Bias2DMean(inplanes)\n",
    "        self.addbias2 = Bias2DMean(width)\n",
    "        self.addbias3 = Bias2DMean(width)\n",
    "\n",
    "        self._scale = nn.Parameter(torch.ones(1))\n",
    "        multiplier = (block_idx + 1)**-(1 / 6) * max_block**(1 / 6)\n",
    "        multiplier = multiplier * (1 - drop_conv)**.5\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) and m.weight.requires_grad:\n",
    "                _, C, H, W = m.weight.shape\n",
    "                stddev = (C * H * W / 2)**-.5\n",
    "                nn.init.normal_(m.weight, std=stddev * multiplier)\n",
    "\n",
    "        self.residual = max_block**-.5\n",
    "        self.identity = block_idx**.5 / (block_idx + 1)**.5\n",
    "\n",
    "        self.downsample = nn.Sequential()\n",
    "        if stride != 1 or inplanes != self.expansion * planes:\n",
    "            if stride == 1:\n",
    "                avgpool = nn.Sequential()\n",
    "            else:\n",
    "                avgpool = nn.AvgPool2d(stride)\n",
    "\n",
    "            self.downsample = nn.Sequential(avgpool, Bias2DMean(num_features=inplanes),\n",
    "                                            nn.Conv2d(inplanes, self.expansion * planes, kernel_size=1, bias=False))\n",
    "            nn.init.kaiming_normal_(self.downsample[2].weight, a=1)\n",
    "\n",
    "        self.drop = nn.Sequential()\n",
    "        if drop_conv > 0.0:\n",
    "            self.drop = nn.Dropout2d(drop_conv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.drop(self.conv1(self.addbias1(x))))\n",
    "        out = F.relu(self.drop(self.conv2(self.addbias2(out))))\n",
    "        out = self.drop(self.conv3(self.addbias3(out)))\n",
    "        out = out * self.residual * self._scale + self.identity * self.downsample(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "    def init_pass(self, x, count):\n",
    "        out = F.relu(self.drop(self.conv1(self.addbias1.init_pass(x, count))))\n",
    "        out = F.relu(self.drop(self.conv2(self.addbias2.init_pass(out, count))))\n",
    "        out = self.drop(self.conv3(self.addbias3.init_pass(out, count)))\n",
    "        out = out * self.residual * self._scale + self.identity * self.downsample(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ReScale(nn.Module):\n",
    "    def __init__(self,\n",
    "                 layers,\n",
    "                 num_classes=1000,\n",
    "                 groups=1,\n",
    "                 width_per_group=64,\n",
    "                 drop_conv=0.0,\n",
    "                 drop_fc=0.0,\n",
    "                 block=Bottleneck,\n",
    "                 input_shapes=(None, None),\n",
    "                 num_flexible_classes=-1):\n",
    "        super(ReScale, self).__init__()\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.num_classes = num_classes\n",
    "        self.input_shapes = input_shapes\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.block_idx = sum(layers) - 1\n",
    "        self.max_depth = sum(layers)\n",
    "        self.num_flexible_classes = num_flexible_classes\n",
    "\n",
    "        # KT TEST SPLIT LEARNING\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.addbias1 = Bias2DMean(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, drop_conv=drop_conv)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, drop_conv=drop_conv)\n",
    "        self.addbias2 = Bias2DMean(512 * block.expansion)\n",
    "        self.drop = nn.Dropout(drop_fc)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        self.mean_pool = nn.AvgPool2d((input_shapes[0] // 32, input_shapes[1] // 32))\n",
    "\n",
    "        # KT TEST SPLIT LEARNING\n",
    "        #nn.init.kaiming_normal_(self.conv1.weight)\n",
    "        nn.init.kaiming_normal_(self.fc.weight, a=1)\n",
    "\n",
    "        if self.num_flexible_classes != -1:\n",
    "            _fixed_sum_layer = torch.zeros(num_classes)\n",
    "            num_unused_classes = num_classes - self.num_flexible_classes\n",
    "            if num_unused_classes > 0:\n",
    "                _fixed_sum_layer[self.num_flexible_classes:] = torch.ones(num_unused_classes) * -10000.0\n",
    "                # initialize bias and weight of unused to 0\n",
    "                self.fc.bias.data[self.num_flexible_classes:] = 0\n",
    "                self.fc.weight.data[self.num_flexible_classes:, :] = 0\n",
    "\n",
    "            # make the fixed_mask not trainable\n",
    "            self.register_buffer(\"fixed_sum_layer\", _fixed_sum_layer)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride=1, drop_conv=0.0):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(\n",
    "                block(self.inplanes,\n",
    "                      planes,\n",
    "                      block_idx=self.block_idx,\n",
    "                      max_block=self.max_depth,\n",
    "                      stride=stride,\n",
    "                      groups=self.groups,\n",
    "                      base_width=self.base_width,\n",
    "                      drop_conv=drop_conv))\n",
    "            self.inplanes = planes * block.expansion\n",
    "            self.block_idx += 1\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # KT TEST SPLIT LEARNING\n",
    "        x = self.conv1(x)\n",
    "        x = self.addbias1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        # x = self.layer1(x)\n",
    "        # x = self.layer2(x)\n",
    "        # x = self.layer3(x)\n",
    "        # x = self.layer4(x)\n",
    "        # x = self.addbias2(x)\n",
    "        #\n",
    "        # x = self.mean_pool(x)\n",
    "        # x = x.squeeze(-1).squeeze(-1)\n",
    "        # x = self.drop(x)\n",
    "        # x = self.fc(x)\n",
    "        # if self.num_flexible_classes != -1:\n",
    "        #     x = x + self.fixed_sum_layer\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def rescale18(num_classes=10, drop_conv=0.0, drop_fc=0.0, **kwargs):\n",
    "    return ReScale([2, 2, 2, 2],\n",
    "                   num_classes=num_classes,\n",
    "                   drop_conv=drop_conv,\n",
    "                   drop_fc=drop_fc,\n",
    "                   groups=1,\n",
    "                   width_per_group=64,\n",
    "                   input_shapes=[32, 32],\n",
    "                   block=BasicBlock,\n",
    "                   **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training hyper parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T03:23:04.983296Z",
     "start_time": "2023-06-12T03:23:04.961770Z"
    }
   },
   "outputs": [],
   "source": [
    "resnet18_client = rescale18().to(device)\n",
    "lr = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(resnet18_client.parameters(), lr = lr, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T05:49:39.257826Z",
     "start_time": "2023-06-12T05:49:39.244543Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training with SambaNova:   0%|                                               | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running epoch  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training with SambaNova: 100%|██████████████████████████████████████| 19/19 [00:01<00:00, 12.51it/s]\n",
      "Training with SambaNova:   0%|                                               | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running epoch  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training with SambaNova: 100%|██████████████████████████████████████| 19/19 [00:01<00:00, 13.06it/s]\n",
      "Training with SambaNova:   0%|                                               | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running epoch  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training with SambaNova: 100%|██████████████████████████████████████| 19/19 [00:01<00:00, 12.78it/s]\n",
      "Training with SambaNova:   0%|                                               | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running epoch  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training with SambaNova: 100%|██████████████████████████████████████| 19/19 [00:01<00:00, 12.04it/s]\n",
      "Training with SambaNova:   0%|                                               | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running epoch  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training with SambaNova: 100%|██████████████████████████████████████| 19/19 [00:01<00:00, 12.90it/s]\n"
     ]
    }
   ],
   "source": [
    "epoch = 5\n",
    "host = '10.9.240.14'\n",
    "port = 8890\n",
    "\n",
    "# host = '10.2.16.246'\n",
    "# port = 18888\n",
    "s1 = socket.socket()\n",
    "s1.connect((host, port)) # establish connection\n",
    "\n",
    "client_weight = copy.deepcopy(resnet18_client.state_dict()) # init weight of client model.\n",
    "\n",
    "msg = {\n",
    "    'epoch': epoch,\n",
    "    'batch_size': batch_size,\n",
    "    'total_batch': total_batch\n",
    "}\n",
    "\n",
    "send_msg(s1, msg) # send 'epoch' and 'batch size' to server\n",
    "\n",
    "resnet18_client.eval()\n",
    "\n",
    "remote_server = recv_msg(s1) # get server's meta information.\n",
    "\n",
    "for epc in range(epoch):\n",
    "    print(\"running epoch \", epc)\n",
    "\n",
    "    target = 0\n",
    "\n",
    "    for i, data in enumerate(tqdm(train_loader, ncols=100, desc='Training with {}'.format(remote_server))):\n",
    "        x, label = data\n",
    "        x = x.to(device)\n",
    "        label = label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        output = resnet18_client(x)\n",
    "        client_output = output.clone().detach().requires_grad_(True)\n",
    "        client_output = client_output.to(torch.bfloat16)\n",
    "\n",
    "        msg = {\n",
    "            'label': label,\n",
    "            'client_output': client_output\n",
    "        }\n",
    "        send_msg(s1, msg) # send label and output(feature) to server\n",
    "        \n",
    "        client_grad = recv_msg(s1) # receive gradaint after the server has completed the back propagation.\n",
    "\n",
    "        output.backward(client_grad) # continue back propagation for client side layers.\n",
    "        optimizer.step()\n",
    "     \n",
    "s1.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sambanova",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
