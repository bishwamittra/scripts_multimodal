{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import socket\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from rescalenet.layers import AvgPool2d, Bias2DMean\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "from rescale_u_shaped import rescale18 as rescale18_server\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from utils import get_metrics_u_shaped, get_metrics_u_shaped_client_side\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T03:23:04.941692Z",
     "start_time": "2023-06-12T03:23:03.944033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client starts from:  0\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train Size (x, y): \n",
      "torch.Size([256, 3, 32, 32]) \n",
      " torch.Size([256])\n",
      "Total Batch Number\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "root_path = 'cifar10_data'\n",
    "\n",
    "# Setup cpu\n",
    "device = 'cpu'\n",
    "\n",
    "# Setup client order\n",
    "client_order = int(0)\n",
    "print('Client starts from: ', client_order)\n",
    "batch_size = 256\n",
    "num_train_data = 5000\n",
    "\n",
    "# Load data\n",
    "from random import shuffle\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "indices = list(range(50000))\n",
    "\n",
    "part_tr = indices[num_train_data * client_order : num_train_data * (client_order + 1)]\n",
    "\n",
    "train_set  = torchvision.datasets.CIFAR10(root=root_path, train=True, download=True, transform=transform)\n",
    "train_set_sub = Subset(train_set, part_tr)\n",
    "train_loader = torch.utils.data.DataLoader(train_set_sub, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR10(root=root_path, train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2, drop_last=True)\n",
    "\n",
    "\n",
    "x_train, y_train = next(iter(train_loader))\n",
    "print('Train Size (x, y): ')\n",
    "print(x_train.size(), '\\n', y_train.size())\n",
    "\n",
    "total_batch = len(train_loader)\n",
    "print('Total Batch Number')\n",
    "print(total_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for communication between client and server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T03:23:04.947320Z",
     "start_time": "2023-06-12T03:23:04.945496Z"
    }
   },
   "outputs": [],
   "source": [
    "def send_msg(sock, msg):\n",
    "    # prefix each message with a 4-byte length in network byte order\n",
    "    msg = pickle.dumps(msg)\n",
    "    msg = struct.pack('>I', len(msg)) + msg\n",
    "    sock.sendall(msg)\n",
    "\n",
    "def recv_msg(sock):\n",
    "    # read message length and unpack it into an integer\n",
    "    raw_msg_len = recv_all(sock, 4)\n",
    "    if not raw_msg_len:\n",
    "        return None\n",
    "    msg_len = struct.unpack('>I', raw_msg_len)[0]\n",
    "    # read the message data\n",
    "    msg =  recv_all(sock, msg_len)\n",
    "    msg = pickle.loads(msg)\n",
    "    return msg\n",
    "\n",
    "def recv_all(sock, n):\n",
    "    # helper function to receive n bytes or return None if EOF is hit\n",
    "    data = b''\n",
    "    while len(data) < n:\n",
    "        packet = sock.recv(n - len(data))\n",
    "        if not packet:\n",
    "            return None\n",
    "        data += packet\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of client side model (input layer only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T03:23:04.957925Z",
     "start_time": "2023-06-12T03:23:04.955930Z"
    }
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(self,\n",
    "                 inplanes: int,\n",
    "                 planes: int,\n",
    "                 block_idx: int,\n",
    "                 max_block: int,\n",
    "                 stride: int = 1,\n",
    "                 groups: int = 1,\n",
    "                 base_width: int = 64,\n",
    "                 drop_conv=0.0) -> None:\n",
    "\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, padding=1, stride=stride, groups=groups, bias=False)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, stride=1, groups=groups, bias=False)\n",
    "\n",
    "        self.addbias1 = Bias2DMean(inplanes)\n",
    "        self.addbias2 = Bias2DMean(planes)\n",
    "\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.stride = stride\n",
    "        self._scale = nn.Parameter(torch.ones(1))\n",
    "        multiplier = (block_idx + 1)**-(1 / 6) * max_block**(1 / 6)\n",
    "        multiplier = multiplier * (1 - drop_conv)**.5\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) and m.weight.requires_grad:\n",
    "                _, C, H, W = m.weight.shape\n",
    "                stddev = (C * H * W / 2)**-.5\n",
    "                nn.init.normal_(m.weight, std=stddev * multiplier)\n",
    "\n",
    "        self.residual = max_block**-.5\n",
    "        self.identity = block_idx**.5 / (block_idx + 1)**.5\n",
    "\n",
    "        self.downsample = nn.Sequential()\n",
    "        if stride != 1 or inplanes != self.expansion * planes:\n",
    "            if stride == 1:\n",
    "                avgpool = nn.Sequential()\n",
    "            else:\n",
    "                avgpool = nn.AvgPool2d(stride)\n",
    "\n",
    "            self.downsample = nn.Sequential(avgpool, Bias2DMean(num_features=inplanes),\n",
    "                                            nn.Conv2d(inplanes, self.expansion * planes, kernel_size=1, bias=False))\n",
    "\n",
    "            nn.init.kaiming_normal_(self.downsample[2].weight, a=1)\n",
    "\n",
    "        self.drop = nn.Sequential()\n",
    "        if drop_conv > 0.0:\n",
    "            self.drop = nn.Dropout2d(drop_conv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Not adding dropout here.\n",
    "        out = F.relu(self.drop(self.conv1(self.addbias1(x))))\n",
    "        out = self.drop(self.conv2(self.addbias2(out)))\n",
    "        out = out * self.residual * self._scale + self.identity * self.downsample(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "    def init_pass(self, x, count):\n",
    "        out = F.relu(self.drop(self.conv1(self.addbias1.init_pass(x, count))))\n",
    "        out = self.drop(self.conv2(self.addbias2.init_pass(out, count)))\n",
    "        out = out * self.residual * self._scale + self.identity * self.downsample(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, block_idx, max_block, stride=1, groups=1, base_width=64, drop_conv=0.0):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(width, width, kernel_size=3, padding=1, stride=stride, groups=groups, bias=False)\n",
    "        self.conv3 = nn.Conv2d(width, planes * self.expansion, kernel_size=1, bias=False)\n",
    "\n",
    "        self.addbias1 = Bias2DMean(inplanes)\n",
    "        self.addbias2 = Bias2DMean(width)\n",
    "        self.addbias3 = Bias2DMean(width)\n",
    "\n",
    "        self._scale = nn.Parameter(torch.ones(1))\n",
    "        multiplier = (block_idx + 1)**-(1 / 6) * max_block**(1 / 6)\n",
    "        multiplier = multiplier * (1 - drop_conv)**.5\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) and m.weight.requires_grad:\n",
    "                _, C, H, W = m.weight.shape\n",
    "                stddev = (C * H * W / 2)**-.5\n",
    "                nn.init.normal_(m.weight, std=stddev * multiplier)\n",
    "\n",
    "        self.residual = max_block**-.5\n",
    "        self.identity = block_idx**.5 / (block_idx + 1)**.5\n",
    "\n",
    "        self.downsample = nn.Sequential()\n",
    "        if stride != 1 or inplanes != self.expansion * planes:\n",
    "            if stride == 1:\n",
    "                avgpool = nn.Sequential()\n",
    "            else:\n",
    "                avgpool = nn.AvgPool2d(stride)\n",
    "\n",
    "            self.downsample = nn.Sequential(avgpool, Bias2DMean(num_features=inplanes),\n",
    "                                            nn.Conv2d(inplanes, self.expansion * planes, kernel_size=1, bias=False))\n",
    "            nn.init.kaiming_normal_(self.downsample[2].weight, a=1)\n",
    "\n",
    "        self.drop = nn.Sequential()\n",
    "        if drop_conv > 0.0:\n",
    "            self.drop = nn.Dropout2d(drop_conv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.drop(self.conv1(self.addbias1(x))))\n",
    "        out = F.relu(self.drop(self.conv2(self.addbias2(out))))\n",
    "        out = self.drop(self.conv3(self.addbias3(out)))\n",
    "        out = out * self.residual * self._scale + self.identity * self.downsample(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "    def init_pass(self, x, count):\n",
    "        out = F.relu(self.drop(self.conv1(self.addbias1.init_pass(x, count))))\n",
    "        out = F.relu(self.drop(self.conv2(self.addbias2.init_pass(out, count))))\n",
    "        out = self.drop(self.conv3(self.addbias3.init_pass(out, count)))\n",
    "        out = out * self.residual * self._scale + self.identity * self.downsample(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ReScale_First(nn.Module):\n",
    "    def __init__(self,\n",
    "                 layers,\n",
    "                 num_classes=1000,\n",
    "                 groups=1,\n",
    "                 width_per_group=64,\n",
    "                 drop_conv=0.0,\n",
    "                 drop_fc=0.0,\n",
    "                 block=Bottleneck,\n",
    "                 input_shapes=(None, None),\n",
    "                 num_flexible_classes=-1):\n",
    "        super(ReScale_First, self).__init__()\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.num_classes = num_classes\n",
    "        self.input_shapes = input_shapes\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.block_idx = sum(layers) - 1\n",
    "        self.max_depth = sum(layers)\n",
    "        self.num_flexible_classes = num_flexible_classes\n",
    "\n",
    "        # KT TEST SPLIT LEARNING\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.addbias1 = Bias2DMean(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        # self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        # self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        # self.layer3 = self._make_layer(block, 256, layers[2], stride=2, drop_conv=drop_conv)\n",
    "        # self.layer4 = self._make_layer(block, 512, layers[3], stride=2, drop_conv=drop_conv)\n",
    "        # self.addbias2 = Bias2DMean(512 * block.expansion)\n",
    "        # self.drop = nn.Dropout(drop_fc)\n",
    "        # self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        self.mean_pool = nn.AvgPool2d((input_shapes[0] // 32, input_shapes[1] // 32))\n",
    "\n",
    "        # KT TEST SPLIT LEARNING\n",
    "        #nn.init.kaiming_normal_(self.conv1.weight)\n",
    "        # nn.init.kaiming_normal_(self.fc.weight, a=1)\n",
    "\n",
    "        if self.num_flexible_classes != -1:\n",
    "            _fixed_sum_layer = torch.zeros(num_classes)\n",
    "            num_unused_classes = num_classes - self.num_flexible_classes\n",
    "            if num_unused_classes > 0:\n",
    "                _fixed_sum_layer[self.num_flexible_classes:] = torch.ones(num_unused_classes) * -10000.0\n",
    "                # initialize bias and weight of unused to 0\n",
    "                # self.fc.bias.data[self.num_flexible_classes:] = 0\n",
    "                # self.fc.weight.data[self.num_flexible_classes:, :] = 0\n",
    "\n",
    "            # make the fixed_mask not trainable\n",
    "            self.register_buffer(\"fixed_sum_layer\", _fixed_sum_layer)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride=1, drop_conv=0.0):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(\n",
    "                block(self.inplanes,\n",
    "                      planes,\n",
    "                      block_idx=self.block_idx,\n",
    "                      max_block=self.max_depth,\n",
    "                      stride=stride,\n",
    "                      groups=self.groups,\n",
    "                      base_width=self.base_width,\n",
    "                      drop_conv=drop_conv))\n",
    "            self.inplanes = planes * block.expansion\n",
    "            self.block_idx += 1\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # KT TEST SPLIT LEARNING\n",
    "        x = self.conv1(x)\n",
    "        x = self.addbias1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        # x = self.layer1(x)\n",
    "        # x = self.layer2(x)\n",
    "        # x = self.layer3(x)\n",
    "        # x = self.layer4(x)\n",
    "        # x = self.addbias2(x)\n",
    "        #\n",
    "        # x = self.mean_pool(x)\n",
    "        # x = x.squeeze(-1).squeeze(-1)\n",
    "        # x = self.drop(x)\n",
    "        # x = self.fc(x)\n",
    "        # if self.num_flexible_classes != -1:\n",
    "        #     x = x + self.fixed_sum_layer\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ReScale_Last(nn.Module):\n",
    "    def __init__(self,\n",
    "                 layers,\n",
    "                 num_classes=1000,\n",
    "                 groups=1,\n",
    "                 width_per_group=64,\n",
    "                 drop_conv=0.0,\n",
    "                 drop_fc=0.0,\n",
    "                 block=Bottleneck,\n",
    "                 input_shapes=(None, None),\n",
    "                 num_flexible_classes=-1):\n",
    "        super(ReScale_Last, self).__init__()\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.num_classes = num_classes\n",
    "        self.input_shapes = input_shapes\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.block_idx = sum(layers) - 1\n",
    "        self.max_depth = sum(layers)\n",
    "        self.num_flexible_classes = num_flexible_classes\n",
    "\n",
    "        # KT TEST SPLIT LEARNING\n",
    "        # self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        # self.addbias1 = Bias2DMean(self.inplanes)\n",
    "        # self.relu = nn.ReLU(inplace=True)\n",
    "        # self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        # self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        # self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        # self.layer3 = self._make_layer(block, 256, layers[2], stride=2, drop_conv=drop_conv)\n",
    "        # self.layer4 = self._make_layer(block, 512, layers[3], stride=2, drop_conv=drop_conv)\n",
    "        # self.addbias2 = Bias2DMean(512 * block.expansion)\n",
    "        # self.drop = nn.Dropout(drop_fc)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        self.mean_pool = nn.AvgPool2d((input_shapes[0] // 32, input_shapes[1] // 32))\n",
    "\n",
    "        # KT TEST SPLIT LEARNING\n",
    "        #nn.init.kaiming_normal_(self.conv1.weight)\n",
    "        nn.init.kaiming_normal_(self.fc.weight, a=1)\n",
    "\n",
    "        if self.num_flexible_classes != -1:\n",
    "            _fixed_sum_layer = torch.zeros(num_classes)\n",
    "            num_unused_classes = num_classes - self.num_flexible_classes\n",
    "            if num_unused_classes > 0:\n",
    "                _fixed_sum_layer[self.num_flexible_classes:] = torch.ones(num_unused_classes) * -10000.0\n",
    "                # initialize bias and weight of unused to 0\n",
    "                self.fc.bias.data[self.num_flexible_classes:] = 0\n",
    "                self.fc.weight.data[self.num_flexible_classes:, :] = 0\n",
    "\n",
    "            # make the fixed_mask not trainable\n",
    "            self.register_buffer(\"fixed_sum_layer\", _fixed_sum_layer)\n",
    "\n",
    "    # def _make_layer(self, block, planes, num_blocks, stride=1, drop_conv=0.0):\n",
    "    #     strides = [stride] + [1] * (num_blocks - 1)\n",
    "    #     layers = []\n",
    "    #     for stride in strides:\n",
    "    #         layers.append(\n",
    "    #             block(self.inplanes,\n",
    "    #                   planes,\n",
    "    #                   block_idx=self.block_idx,\n",
    "    #                   max_block=self.max_depth,\n",
    "    #                   stride=stride,\n",
    "    #                   groups=self.groups,\n",
    "    #                   base_width=self.base_width,\n",
    "    #                   drop_conv=drop_conv))\n",
    "    #         self.inplanes = planes * block.expansion\n",
    "    #         self.block_idx += 1\n",
    "    #     return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # KT TEST SPLIT LEARNING\n",
    "        # x = self.conv1(x)\n",
    "        # x = self.addbias1(x)\n",
    "        # x = self.relu(x)\n",
    "        # x = self.maxpool(x)\n",
    "        # x = self.layer1(x)\n",
    "        # x = self.layer2(x)\n",
    "        # x = self.layer3(x)\n",
    "        # x = self.layer4(x)\n",
    "        # x = self.addbias2(x)\n",
    "        #\n",
    "        # x = self.mean_pool(x)\n",
    "        # x = x.squeeze(-1).squeeze(-1)\n",
    "        # x = self.drop(x)\n",
    "        x = self.fc(x)\n",
    "        if self.num_flexible_classes != -1:\n",
    "            x = x + self.fixed_sum_layer\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def rescale18_first(num_classes=10, drop_conv=0.0, drop_fc=0.0, **kwargs):\n",
    "    return ReScale_First([2, 2, 2, 2],\n",
    "                   num_classes=num_classes,\n",
    "                   drop_conv=drop_conv,\n",
    "                   drop_fc=drop_fc,\n",
    "                   groups=1,\n",
    "                   width_per_group=64,\n",
    "                   input_shapes=[32, 32],\n",
    "                   block=BasicBlock,\n",
    "                   **kwargs)\n",
    "\n",
    "def rescale18_last(num_classes=10, drop_conv=0.0, drop_fc=0.0, **kwargs):\n",
    "    return ReScale_Last([2, 2, 2, 2],\n",
    "                   num_classes=num_classes,\n",
    "                   drop_conv=drop_conv,\n",
    "                   drop_fc=drop_fc,\n",
    "                   groups=1,\n",
    "                   width_per_group=64,\n",
    "                   input_shapes=[32, 32],\n",
    "                   block=BasicBlock,\n",
    "                   **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training hyper parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T03:23:04.983296Z",
     "start_time": "2023-06-12T03:23:04.961770Z"
    }
   },
   "outputs": [],
   "source": [
    "resnet18_client_first = rescale18_first().to(device)\n",
    "lr = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_first = optim.SGD(resnet18_client_first.parameters(), lr = lr, momentum = 0.9)\n",
    "\n",
    "\n",
    "resnet18_client_last = rescale18_last().to(device)\n",
    "optimizer_last = optim.SGD(resnet18_client_last.parameters(), lr = lr, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T05:49:39.257826Z",
     "start_time": "2023-06-12T05:49:39.244543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10\n",
      "Train Loss: 0.0004, Train Acc: 0.2208, Train AUC: 0.7158, Train Bal Acc: 0.2185\n",
      "Test Loss: 0.0002, Test Acc: 0.2073, Test AUC: 0.7058, Test Bal Acc: 0.2073\n",
      "\n",
      "Epoch: 1/10\n",
      "Train Loss: 0.0004, Train Acc: 0.286, Train AUC: 0.7501, Train Bal Acc: 0.2852\n",
      "Test Loss: 0.0002, Test Acc: 0.2795, Test AUC: 0.7441, Test Bal Acc: 0.2795\n",
      "\n",
      "Epoch: 2/10\n",
      "Train Loss: 0.0004, Train Acc: 0.2911, Train AUC: 0.7763, Train Bal Acc: 0.2897\n",
      "Test Loss: 0.0002, Test Acc: 0.2830, Test AUC: 0.7701, Test Bal Acc: 0.2829\n",
      "\n",
      "Epoch: 3/10\n",
      "Train Loss: 0.0004, Train Acc: 0.3363, Train AUC: 0.8019, Train Bal Acc: 0.3388\n",
      "Test Loss: 0.0002, Test Acc: 0.3279, Test AUC: 0.7938, Test Bal Acc: 0.3280\n",
      "\n",
      "Epoch: 4/10\n",
      "Train Loss: 0.0004, Train Acc: 0.3565, Train AUC: 0.8210, Train Bal Acc: 0.3570\n",
      "Test Loss: 0.0002, Test Acc: 0.3381, Test AUC: 0.8119, Test Bal Acc: 0.3382\n",
      "\n",
      "Epoch: 5/10\n",
      "Train Loss: 0.0003, Train Acc: 0.3705, Train AUC: 0.8289, Train Bal Acc: 0.3720\n",
      "Test Loss: 0.0002, Test Acc: 0.3538, Test AUC: 0.8161, Test Bal Acc: 0.3538\n",
      "\n",
      "Epoch: 6/10\n",
      "Train Loss: 0.0003, Train Acc: 0.405, Train AUC: 0.8476, Train Bal Acc: 0.4059\n",
      "Test Loss: 0.0002, Test Acc: 0.3922, Test AUC: 0.8355, Test Bal Acc: 0.3923\n",
      "\n",
      "Epoch: 7/10\n",
      "Train Loss: 0.0003, Train Acc: 0.4266, Train AUC: 0.8580, Train Bal Acc: 0.4272\n",
      "Test Loss: 0.0002, Test Acc: 0.4042, Test AUC: 0.8448, Test Bal Acc: 0.4043\n",
      "\n",
      "Epoch: 8/10\n",
      "Train Loss: 0.0003, Train Acc: 0.4482, Train AUC: 0.8665, Train Bal Acc: 0.4461\n",
      "Test Loss: 0.0002, Test Acc: 0.4139, Test AUC: 0.8514, Test Bal Acc: 0.4139\n",
      "\n",
      "Epoch: 9/10\n",
      "Train Loss: 0.0003, Train Acc: 0.4459, Train AUC: 0.8707, Train Bal Acc: 0.4465\n",
      "Test Loss: 0.0002, Test Acc: 0.4234, Test AUC: 0.8552, Test Bal Acc: 0.4234\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch = 10\n",
    "host = '10.9.240.14'\n",
    "port = 8890\n",
    "\n",
    "# host = '10.2.16.246'\n",
    "# port = 18888\n",
    "s1 = socket.socket()\n",
    "s1.connect((host, port)) # establish connection\n",
    "\n",
    "\n",
    "msg = {\n",
    "    'epoch': epoch,\n",
    "    'batch_size': batch_size,\n",
    "    'total_batch': total_batch\n",
    "}\n",
    "\n",
    "send_msg(s1, msg) # send 'epoch' and 'batch size' to server\n",
    "\n",
    "\n",
    "\n",
    "remote_server = recv_msg(s1) # get server's meta information.\n",
    "\n",
    "for epc in range(epoch):\n",
    "    # print(\"running epoch \", epc)\n",
    "    resnet18_client_first.train()\n",
    "    resnet18_client_last.train()\n",
    "\n",
    "    for i, data in enumerate(tqdm(train_loader, ncols=100, desc='Training with {}'.format(remote_server), disable=True)):\n",
    "        x, label = data\n",
    "        x = x.to(device)\n",
    "        label = label.to(device)\n",
    "        optimizer_first.zero_grad()\n",
    "        optimizer_last.zero_grad()\n",
    "\n",
    "\n",
    "        output = resnet18_client_first(x)\n",
    "        client_output_first = output.clone().detach().requires_grad_(True)\n",
    "        client_output_first = client_output_first.to(torch.bfloat16)\n",
    "\n",
    "        msg = {\n",
    "            # 'label': label,\n",
    "            'client_output': client_output_first\n",
    "        }\n",
    "        send_msg(s1, msg) # send label and output(feature) to server\n",
    "        \n",
    "        \n",
    "        rmsg = recv_msg(s1)\n",
    "        server_output_samba = rmsg['server_output']\n",
    "        server_output = server_output_samba.to(device)\n",
    "\n",
    "        output_last = resnet18_client_last(server_output)\n",
    "\n",
    "        loss = criterion(output_last, label)\n",
    "        # print(loss.item())\n",
    "        loss.backward()\n",
    "        \n",
    "        msg = {\n",
    "            'server_grad': server_output_samba.grad.clone().detach()\n",
    "        }\n",
    "        send_msg(s1, msg) # send server's gradient to server\n",
    "        optimizer_first.step()\n",
    "\n",
    "\n",
    "        rmsg = recv_msg(s1)\n",
    "        client_output_first.backward(rmsg['client_output_grad'])\n",
    "        optimizer_last.step()\n",
    "\n",
    "\n",
    "    # rmsg = recv_msg(s1)\n",
    "    # server_model_state_dict = rmsg['server model']\n",
    "    # rescalenet_server = rescale18_server(num_classes=10, input_shapes=(32, 32)).to(device)\n",
    "    # rescalenet_server.load_state_dict(server_model_state_dict)\n",
    "\n",
    "    # # print(rescalenet_server.state_dict().keys())\n",
    "    # # print(rescalenet_server.state_dict()['layer3.0.conv1.weight'])\n",
    "\n",
    "    # train_loss, train_acc, train_auc, train_bal_acc = get_metrics_u_shaped(rescalenet_server, resnet18_client_first, resnet18_client_last, train_loader, criterion, device)\n",
    "    # test_loss, test_acc, test_auc, test_bal_acc = get_metrics_u_shaped(rescalenet_server, resnet18_client_first, resnet18_client_last, test_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch: {epc+1}/{epoch}\")\n",
    "    train_loss, train_acc, train_auc, train_bal_acc = get_metrics_u_shaped_client_side(resnet18_client_first, resnet18_client_last, train_loader, send_msg, recv_msg, s1, criterion, device)\n",
    "    test_loss, test_acc, test_auc, test_bal_acc = get_metrics_u_shaped_client_side(resnet18_client_first, resnet18_client_last, test_loader, send_msg, recv_msg, s1, criterion, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {round(train_acc, 4)}, Train AUC: {train_auc:.4f}, Train Bal Acc: {train_bal_acc:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}, Test AUC: {test_auc:.4f}, Test Bal Acc: {test_bal_acc:.4f}\")\n",
    "    print()    \n",
    "     \n",
    "s1.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sambanova",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
