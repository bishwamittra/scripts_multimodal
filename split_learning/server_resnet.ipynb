{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/home/bishwa/mambaforge-pypy3/envs/multimodal/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "server_name = 'SERVER_001'\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "import socket\n",
    "import struct\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "import time\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.init as init\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup CUDA\n",
    "seed_num = 777\n",
    "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# torch.manual_seed(seed_num)\n",
    "# if device == \"cuda:0\":\n",
    "#     torch.cuda.manual_seed_all(seed_num)\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_msg(sock, msg):\n",
    "    # prefix each message with a 4-byte length in network byte order\n",
    "    msg = pickle.dumps(msg)\n",
    "    l_send = len(msg)\n",
    "    msg = struct.pack('>I', l_send) + msg\n",
    "    sock.sendall(msg)\n",
    "    return l_send\n",
    "\n",
    "def recv_msg(sock):\n",
    "    # read message length and unpack it into an integer\n",
    "    raw_msg_len = recv_all(sock, 4)\n",
    "    if not raw_msg_len:\n",
    "        return None\n",
    "    msg_len = struct.unpack('>I', raw_msg_len)[0]\n",
    "    # read the message data\n",
    "    msg =  recv_all(sock, msg_len)\n",
    "    msg = pickle.loads(msg)\n",
    "    return msg, msg_len\n",
    "\n",
    "def recv_all(sock, n):\n",
    "    # helper function to receive n bytes or return None if EOF is hit\n",
    "    data = b''\n",
    "    while len(data) < n:\n",
    "        packet = sock.recv(n - len(data))\n",
    "        if not packet:\n",
    "            return None\n",
    "        data += packet\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model definition of server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' ResNet '''\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, norm='instancenorm'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.norm = norm\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.GroupNorm(self.expansion*planes, self.expansion*planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, norm='instancenorm'):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.norm = norm\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.GroupNorm(self.expansion*planes, self.expansion*planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.GroupNorm(self.expansion*planes, self.expansion*planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class Bottleneck_server(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, norm='instancenorm'):\n",
    "        super(Bottleneck_server, self).__init__()\n",
    "        self.norm = norm\n",
    "        # self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        # self.bn1 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.GroupNorm(planes, planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.GroupNorm(self.expansion*planes, self.expansion*planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.GroupNorm(self.expansion*planes, self.expansion*planes, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(x)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    # def __init__(self, block, num_blocks, channel=3, num_classes=10, norm='instancenorm'):\n",
    "    def __init__(self, block, block_server, num_blocks, num_classes=10, norm='instancenorm'):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        self.norm = norm\n",
    "\n",
    "        # self.conv1 = nn.Conv2d(channel, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        # self.bn1 = nn.GroupNorm(64, 64, affine=True) if self.norm == 'instancenorm' else nn.BatchNorm2d(64)\n",
    "        # self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer1 = self._make_layer_server(block, block_server, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.classifier = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride, self.norm))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _make_layer_server(self, block, block_server, planes, num_blocks, stride):\n",
    "        # strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        layers.append(block_server(self.in_planes, planes, 1))\n",
    "        self.in_planes = planes * block.expansion\n",
    "        layers.append(block(self.in_planes, planes, 1))\n",
    "        self.in_planes = planes * block.expansion\n",
    "        layers.append(block(self.in_planes, planes, 1))\n",
    "        self.in_planes = planes * block.expansion\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    # def embed(self, x):\n",
    "    #     out = F.relu(self.bn1(self.conv1(x)))\n",
    "    #     out = self.layer1(out)\n",
    "    #     out = self.layer2(out)\n",
    "    #     out = self.layer3(out)\n",
    "    #     out = self.layer4(out)\n",
    "    #     out = F.avg_pool2d(out, 4)\n",
    "    #     out = out.view(out.size(0), -1)\n",
    "    #     return out\n",
    "\n",
    "\n",
    "\n",
    "def ResNet50(num_classes):\n",
    "    return ResNet(Bottleneck, Bottleneck_server, [3,4,6,3], num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_server =  ResNet50(num_classes=10).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 0.001\n",
    "optimizer = optim.SGD(resnet_server.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck_server(\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): GroupNorm(64, 64, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(256, 256, eps=1e-05, affine=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): GroupNorm(256, 256, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(64, 64, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): GroupNorm(64, 64, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(256, 256, eps=1e-05, affine=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(64, 64, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): GroupNorm(64, 64, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(256, 256, eps=1e-05, affine=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): GroupNorm(128, 128, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(256, 256, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): GroupNorm(256, 256, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(1024, 1024, eps=1e-05, affine=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): GroupNorm(1024, 1024, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(256, 256, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): GroupNorm(256, 256, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(1024, 1024, eps=1e-05, affine=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(256, 256, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): GroupNorm(256, 256, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(1024, 1024, eps=1e-05, affine=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(256, 256, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): GroupNorm(256, 256, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(1024, 1024, eps=1e-05, affine=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(256, 256, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): GroupNorm(256, 256, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(1024, 1024, eps=1e-05, affine=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(256, 256, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): GroupNorm(256, 256, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(1024, 1024, eps=1e-05, affine=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(2048, 2048, eps=1e-05, affine=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): GroupNorm(2048, 2048, eps=1e-05, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(2048, 2048, eps=1e-05, affine=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): GroupNorm(2048, 2048, eps=1e-05, affine=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=2048, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to:  ('10.2.143.109', 44298)\n",
      "received epoch:  1 6250\n",
      "Start training @  Thu Oct  5 11:10:15 2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training with SERVER_001:   0%|                                            | 0/6250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1, Batch: 1/6250, Train Loss: 2.6 Train Accuracy: 0.12\n",
      "Contribution from SERVER_001 is done\n",
      "Contribution duration is: 0.29991793632507324 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# host = '10.2.144.188'\n",
    "# host = '10.9.240.14'\n",
    "host = '10.2.143.109'\n",
    "port = 10081\n",
    "\n",
    "s = socket.socket()\n",
    "s.bind((host, port))\n",
    "s.listen(5)\n",
    "\n",
    "conn, addr = s.accept()\n",
    "print(\"Connected to: \", addr)\n",
    "\n",
    "# read epoch\n",
    "rmsg, data_size = recv_msg(conn) # receive total bach number and epoch from client.\n",
    "\n",
    "epoch = rmsg['epoch']\n",
    "num_batch = rmsg['total_batch']\n",
    "\n",
    "print(\"received epoch: \", rmsg['epoch'], rmsg['total_batch'])\n",
    "\n",
    "send_msg(conn, server_name) # send server meta information.\n",
    "\n",
    "# Start training\n",
    "start_time = time.time()\n",
    "print(\"Start training @ \", time.asctime())\n",
    "\n",
    "for epc in range(epoch):\n",
    "    init = 0\n",
    "    for i in tqdm(range(num_batch), ncols = 100, desc='Training with {}'.format(server_name)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        msg, data_size = recv_msg(conn) # receives label and feature from client.\n",
    "        \n",
    "        # label\n",
    "        label = msg['label']\n",
    "        label = label.clone().detach().long().to(device) # conversion between gpu and cpu.\n",
    "        \n",
    "        # feature\n",
    "        client_output_cpu = msg['client_output']\n",
    "        client_output = client_output_cpu.to(device)\n",
    "\n",
    "        # forward propagation\n",
    "        output = resnet_server(client_output)\n",
    "        loss = criterion(output, label) # compute cross-entropy loss\n",
    "        loss.backward() # backward propagation\n",
    "        \n",
    "        # send gradient to client\n",
    "        msg = client_output_cpu.grad.clone().detach()\n",
    "        data_size = send_msg(conn, msg)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        if i % 100 == 0:\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct = (predicted == label).sum().item()\n",
    "            accuracy = correct / len(label)\n",
    "            print(f'Epoch: {epc+1}/{epoch}, Batch: {i+1}/{num_batch}, Train Loss: {round(loss.item(), 2)} Train Accuracy: {round(accuracy, 2)}')\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "print('Contribution from {} is done'.format(server_name))\n",
    "print('Contribution duration is: {} seconds'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
