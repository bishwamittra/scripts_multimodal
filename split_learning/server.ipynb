{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_name = 'SERVER_001'\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "import socket\n",
    "import struct\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "import time\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.init as init\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup CUDA\n",
    "seed_num = 777\n",
    "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# torch.manual_seed(seed_num)\n",
    "# if device == \"cuda:0\":\n",
    "#     torch.cuda.manual_seed_all(seed_num)\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_msg(sock, msg):\n",
    "    # prefix each message with a 4-byte length in network byte order\n",
    "    msg = pickle.dumps(msg)\n",
    "    l_send = len(msg)\n",
    "    msg = struct.pack('>I', l_send) + msg\n",
    "    sock.sendall(msg)\n",
    "    return l_send\n",
    "\n",
    "def recv_msg(sock):\n",
    "    # read message length and unpack it into an integer\n",
    "    raw_msg_len = recv_all(sock, 4)\n",
    "    if not raw_msg_len:\n",
    "        return None\n",
    "    msg_len = struct.unpack('>I', raw_msg_len)[0]\n",
    "    # read the message data\n",
    "    msg =  recv_all(sock, msg_len)\n",
    "    msg = pickle.loads(msg)\n",
    "    return msg, msg_len\n",
    "\n",
    "def recv_all(sock, n):\n",
    "    # helper function to receive n bytes or return None if EOF is hit\n",
    "    data = b''\n",
    "    while len(data) < n:\n",
    "        packet = sock.recv(n - len(data))\n",
    "        if not packet:\n",
    "            return None\n",
    "        data += packet\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model definition of server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == 'A':\n",
    "                \"\"\"\n",
    "                For CIFAR10 ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(lambda x:\n",
    "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            elif option == 'B':\n",
    "                self.shortcut = nn.Sequential(\n",
    "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                     nn.BatchNorm2d(self.expansion * planes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "    \n",
    "class BasicBlock_server(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
    "        super(BasicBlock_server, self).__init__()\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == 'A':\n",
    "                \"\"\"\n",
    "                For CIFAR10 ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(lambda x:\n",
    "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "            elif option == 'B':\n",
    "                self.shortcut = nn.Sequential(\n",
    "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                     nn.BatchNorm2d(self.expansion * planes)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.bn2(self.conv2(x))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, block_server, num_blocks, num_classes=10, ):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "        self.layer1 = self._make_layer_server(block, block_server, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer_server(self, block, block_server, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        layers.append(block_server(self.in_planes, planes, 1))\n",
    "        self.in_planes = planes * block.expansion\n",
    "        layers.append(block(self.in_planes, planes, 1))\n",
    "        self.in_planes = planes * block.expansion\n",
    "        layers.append(block(self.in_planes, planes, 1))\n",
    "        self.in_planes = planes * block.expansion\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet20():\n",
    "    return ResNet(BasicBlock, BasicBlock_server, [3, 3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet20_server =  resnet20().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 0.001\n",
    "optimizer = optim.SGD(resnet20_server.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to:  ('10.9.240.14', 43922)\n",
      "received epoch:  1 12500\n",
      "Start training @  Mon Oct  2 12:00:06 2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training with SERVER_001: 100%|███████████████████████████████| 12500/12500 [04:22<00:00, 47.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contribution from SERVER_001 is done\n",
      "Contribution duration is: 262.7710027694702 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# host = '10.2.144.188'\n",
    "host = '10.9.240.14'\n",
    "port = 10081\n",
    "\n",
    "s = socket.socket()\n",
    "s.bind((host, port))\n",
    "s.listen(5)\n",
    "\n",
    "conn, addr = s.accept()\n",
    "print(\"Connected to: \", addr)\n",
    "\n",
    "# read epoch\n",
    "rmsg, data_size = recv_msg(conn) # receive total bach number and epoch from client.\n",
    "\n",
    "epoch = rmsg['epoch']\n",
    "batch_size = rmsg['total_batch']\n",
    "\n",
    "print(\"received epoch: \", rmsg['epoch'], rmsg['total_batch'])\n",
    "\n",
    "send_msg(conn, server_name) # send server meta information.\n",
    "\n",
    "# Start training\n",
    "start_time = time.time()\n",
    "print(\"Start training @ \", time.asctime())\n",
    "\n",
    "for epc in range(epoch):\n",
    "    init = 0\n",
    "    for i in tqdm(range(batch_size), ncols = 100, desc='Training with {}'.format(server_name)):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        msg, data_size = recv_msg(conn) # receives label and feature from client.\n",
    "        \n",
    "        # label\n",
    "        label = msg['label']\n",
    "        label = label.clone().detach().long().to(device) # conversion between gpu and cpu.\n",
    "        \n",
    "        # feature\n",
    "        client_output_cpu = msg['client_output']\n",
    "        client_output = client_output_cpu.to(device)\n",
    "\n",
    "        # forward propagation\n",
    "        output = resnet20_server(client_output)\n",
    "        loss = criterion(output, label) # compute cross-entropy loss\n",
    "        loss.backward() # backward propagation\n",
    "        \n",
    "        # send gradient to client\n",
    "        msg = client_output_cpu.grad.clone().detach()\n",
    "        data_size = send_msg(conn, msg)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "\n",
    "print('Contribution from {} is done'.format(server_name))\n",
    "print('Contribution duration is: {} seconds'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
